{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23ece6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.loadbinary_GraphExecutorFactory\n",
      "Load_Executable\n",
      "GetNumOfPrimitives\n",
      "GetGlobalFields\n",
      "GetNumOfGlobals\n",
      "module.loadbinary_VMExecutable\n",
      "profiling.FromJSON\n",
      "profiling.AsJSON\n",
      "profiling.AsCSV\n",
      "profiling.AsTable\n",
      "TVMArrayAllocWithScope\n",
      "ModuleSaveToFile\n",
      "ModuleGetTypeKey\n",
      "ModuleGetImport\n",
      "module.loadbinary_metadata\n",
      "GetADTTag\n",
      "ArraySize\n",
      "Array\n",
      "GetDeviceAttr\n",
      "CreateLLVMCrtMetadataModule\n",
      "_datatype_register\n",
      "GetFFIString\n",
      "module.loadfile_so\n",
      "profiling.DeviceWrapper\n",
      "MapItems\n",
      "TVMSetStream\n",
      "ShapeTuple\n",
      "DumpTypeTable\n",
      "module.loadfile_ll\n",
      "GetPrimitiveFields\n",
      "config_threadpool\n",
      "module.loadfile_VMExecutable\n",
      "module.loadfile_stackvm\n",
      "ModuleGetSource\n",
      "ModuleImportsSize\n",
      "RuntimeEnabled\n",
      "Tuple\n",
      "LoadParams\n",
      "Map\n",
      "SystemLib\n",
      "MapSize\n",
      "GetADTSize\n",
      "ModuleLoadFromFile\n",
      "GetShapeTupleSize\n",
      "CSourceModuleCreate\n",
      "String\n",
      "MapCount\n",
      "_datatype_get_type_name\n",
      "_VirtualMachine\n",
      "MapGetItem\n",
      "ArrayGetItem\n",
      "ObjectPtrHash\n",
      "SaveParams\n",
      "GetShapeTupleElem\n",
      "ModulePackImportsToC\n",
      "_datatype_get_type_registered\n",
      "module.loadbinary_GraphRuntimeFactory\n",
      "GetADTFields\n",
      "InterfaceCCreate\n",
      "_datatype_get_type_code\n",
      "SourceModuleCreate\n",
      "RPCTimeEvaluator\n",
      "_VirtualMachineDebug\n",
      "ModulePackImportsToLLVM\n",
      "CreateCSourceCrtMetadataModule\n",
      "ADT\n",
      "StructuralEqual\n",
      "LoadJSON\n",
      "NodeListAttrNames\n",
      "NodeGetAttr\n",
      "SaveJSON\n",
      "AsRepr\n",
      "MakeNode\n",
      "_const\n",
      "LargeUIntImm\n",
      "StructuralHash\n",
      "FromJSON\n",
      "AsJSON\n",
      "AsCSV\n",
      "AsTable\n",
      "DeviceWrapper\n",
      "NodeSetSpan\n",
      "Module_GetGlobalVar\n",
      "PrettyPrint\n",
      "AsText\n",
      "EnvFuncGetPackedFunc\n",
      "IncompleteType\n",
      "FuncType\n",
      "TypeVar\n",
      "PrimType\n",
      "TensorAffineType\n",
      "Module_LookupDef\n",
      "Module_Import\n",
      "Range_from_min_extent\n",
      "Module_GetGlobalVars\n",
      "EnvFuncGet\n",
      "TupleType\n",
      "TensorType\n",
      "DebugPrint\n",
      "OpSetAttrsTypeKey\n",
      "BaseFuncWithAttr\n",
      "GlobalVar\n",
      "Module_GetGlobalTypeVar\n",
      "TypeCall\n",
      "OpAddTypeRel\n",
      "FloatImm\n",
      "Module_ContainGlobalTypeVar\n",
      "Module_FromExpr\n",
      "Module_ImportFromStd\n",
      "Module_AddDef\n",
      "Module_Add\n",
      "Constructor\n",
      "TypeData\n",
      "DictAttrsGetDict\n",
      "RegisterOpAttr\n",
      "AttrsListFieldInfo\n",
      "BaseFunc_Attrs\n",
      "BaseFuncCopy\n",
      "OpGetAttr\n",
      "IRModule\n",
      "Module_GetGlobalTypeVars\n",
      "Module_ContainGlobalVar\n",
      "Module_Lookup\n",
      "OpResetAttr\n",
      "Module_Lookup_str\n",
      "Module_LookupDef_str\n",
      "Module_Update\n",
      "Module_LookupTag\n",
      "Module_UpdateFunction\n",
      "EnvFuncCall\n",
      "ListOpNames\n",
      "OpSetAttr\n",
      "RegisterOp\n",
      "GetOp\n",
      "RegisterOpLowerIntrinsic\n",
      "SourceName\n",
      "TypeRelation\n",
      "RelayRefType\n",
      "Span\n",
      "Range\n",
      "TextPrinter\n",
      "TupleAffineType\n",
      "OpSetSupportLevel\n",
      "IntImm\n",
      "GlobalTypeVar\n",
      "OpSetNumInputs\n",
      "PointerType\n",
      "OpAddArgument\n",
      "OverrideInstruments\n",
      "GetCurrentPassContext\n",
      "ListConfigs\n",
      "ExitPassContext\n",
      "PrintIR\n",
      "RunPass\n",
      "Info\n",
      "EnterPassContext\n",
      "Sequential\n",
      "MakeModulePass\n",
      "PassContext\n",
      "PassInfo\n",
      "MakePassTimingInstrument\n",
      "PassInstrument\n",
      "RenderTimePassProfiles\n",
      "DiagnosticRendererRender\n",
      "Default\n",
      "DiagnosticContextRender\n",
      "DefaultRenderer\n",
      "GetRenderer\n",
      "ClearRenderer\n",
      "DiagnosticRenderer\n",
      "Diagnostic\n",
      "DiagnosticContext\n",
      "Emit\n",
      "schedule.ScheduleEnterPostproc\n",
      "GT\n",
      "Mod\n",
      "Mul\n",
      "LayoutNdim\n",
      "LayoutFactorOf\n",
      "Layout\n",
      "round\n",
      "BufferVStore\n",
      "BufferStorageScope\n",
      "BufferVLoad\n",
      "transform.StorageFlatten\n",
      "BufferAccessPtr\n",
      "FloorDiv\n",
      "schedule.TracedSchedule\n",
      "schedule.ScheduleGetBlock\n",
      "Or\n",
      "LayoutIndexOf\n",
      "StringImm\n",
      "analysis.detect_buffer_access_lca\n",
      "schedule.ScheduleBind\n",
      "Not\n",
      "schedule.ScheduleCopy\n",
      "BijectiveLayoutBackwardShape\n",
      "schedule.ScheduleStorageAlign\n",
      "analysis.calculate_workspace_bytes\n",
      "EQ\n",
      "BijectiveLayoutForwardShape\n",
      "GE\n",
      "LE\n",
      "transform.BF16Legalize\n",
      "schedule.ScheduleComputeAt\n",
      "SizeVar\n",
      "isfinite\n",
      "SeqStmt\n",
      "schedule.ScheduleGetLoops\n",
      "analysis.GetBlockReadWriteRegion\n",
      "BufferStore\n",
      "BijectiveLayoutBackwardIndex\n",
      "transform.InferFragment\n",
      "transform.CombineContextCall\n",
      "Any\n",
      "ProducerRealize\n",
      "CommReducerCombine\n",
      "_OpOr\n",
      "schedule.ScheduleReverseComputeAt\n",
      "AttrStmt\n",
      "Sub\n",
      "transform.NarrowDataType\n",
      "Var\n",
      "transform.RemoveNoOp\n",
      "_OpLE\n",
      "_OpNE\n",
      "schedule.ScheduleRFactor\n",
      "Min\n",
      "schedule.ScheduleStateReplace\n",
      "IterVar\n",
      "AssertStmt\n",
      "Div\n",
      "_OpGT\n",
      "_OpTruncDiv\n",
      "transform.SplitHostDevice\n",
      "Broadcast\n",
      "bitwise_xor\n",
      "transform.LowerDeviceStorageAccessInfo\n",
      "BijectiveLayout\n",
      "_OpAnd\n",
      "ModelLibraryFormatPrinter\n",
      "analysis.expr_deep_equal\n",
      "_OpDiv\n",
      "And\n",
      "BijectiveLayoutForwardIndex\n",
      "schedule.ScheduleDecomposeReduction\n",
      "LT\n",
      "transform.UnifyThreadBinding\n",
      "_OpIndexMod\n",
      "schedule.TraceSimplified\n",
      "transform.InstrumentBoundCheckers\n",
      "analysis.verify_gpu_code\n",
      "schedule.TraceWithDecision\n",
      "abs\n",
      "Cast\n",
      "FloorMod\n",
      "schedule.StmtSRefStmt\n",
      "transform.LowerTVMBuiltin\n",
      "_OpFloorDiv\n",
      "Select\n",
      "transform.Simplify\n",
      "transform.VerifyMemory\n",
      "schedule.TraceApplyJSONToSchedule\n",
      "schedule.ScheduleGetTrace\n",
      "transform.VerifyGPUCode\n",
      "transform.CreatePrimFuncPass\n",
      "bitwise_and\n",
      "_OpMod\n",
      "schedule.ScheduleComputeInline\n",
      "analysis.verify_memory\n",
      "Add\n",
      "Buffer\n",
      "schedule.ScheduleStateGetSRef\n",
      "Substitute\n",
      "ceil\n",
      "PostOrderVisit\n",
      "schedule.StmtSRefParent\n",
      "_OpIfThenElse\n",
      "analysis.verify_ssa\n",
      "transform.InjectVirtualThread\n",
      "_OpGE\n",
      "transform.LowerInitBlock\n",
      "transform.VerifySSA\n",
      "transform.CoProcSync\n",
      "Store\n",
      "schedule.LoopRV\n",
      "right_shift\n",
      "transform.TextureFlatten\n",
      "schedule.ConcreteSchedule\n",
      "NE\n",
      "schedule.TraceAppend\n",
      "Load\n",
      "schedule.ScheduleReverseComputeInline\n",
      "Ramp\n",
      "Shuffle\n",
      "CommReducer\n",
      "Reduce\n",
      "analysis.GetBlockAccessRegion\n",
      "transform.RewriteUnsafeSelect\n",
      "BufferLoad\n",
      "ProducerLoad\n",
      "PrimFunc\n",
      "Specialize\n",
      "LetStmt\n",
      "For\n",
      "schedule.BlockRV\n",
      "nearbyint\n",
      "While\n",
      "transform.ConvertBlocksToOpaque\n",
      "ProducerStore\n",
      "Allocate\n",
      "Prefetch\n",
      "Evaluate\n",
      "BufferRealize\n",
      "BufferRegion\n",
      "MatchBufferRegion\n",
      "Let\n",
      "BlockRealize\n",
      "Max\n",
      "IRTransform\n",
      "min_value\n",
      "schedule.RegisterReducer\n",
      "max_value\n",
      "isnan\n",
      "isinf\n",
      "floor\n",
      "trunc\n",
      "_cast\n",
      "_OpAdd\n",
      "_OpIndexDiv\n",
      "_OpFloorMod\n",
      "_OpTruncMod\n",
      "_OpPow\n",
      "_OpMax\n",
      "_OpEQ\n",
      "schedule.Instruction\n",
      "bitwise_or\n",
      "const_true\n",
      "schedule.StmtSRefInlineMark\n",
      "schedule.BlockScopeGetDepsBySrc\n",
      "Block\n",
      "transform.HoistIfThenElse\n",
      "transform.FlattenBuffer\n",
      "schedule.ScheduleGetMod\n",
      "schedule.ScheduleGetState\n",
      "transform.SkipAssert\n",
      "schedule.ScheduleSeed\n",
      "schedule.ScheduleForkSeed\n",
      "schedule.ScheduleGet\n",
      "schedule.ScheduleGetSRef\n",
      "schedule.ScheduleRemoveRV\n",
      "schedule.ScheduleSampleCategorical\n",
      "schedule.ScheduleGetChildBlocks\n",
      "schedule.ScheduleGetProducers\n",
      "schedule.ScheduleFuse\n",
      "IfThenElse\n",
      "schedule.ScheduleReorder\n",
      "schedule.ScheduleParallel\n",
      "schedule.ScheduleVectorize\n",
      "schedule.ScheduleUnroll\n",
      "schedule.ScheduleCacheRead\n",
      "schedule.ScheduleCacheWrite\n",
      "schedule.ScheduleStateGetBlockScope\n",
      "transform.LoopPartition\n",
      "transform.LowerIntrin\n",
      "_OpMul\n",
      "schedule.ScheduleStateGetCachedFlags\n",
      "schedule.Trace\n",
      "schedule.TraceApplyToSchedule\n",
      "schedule.TraceGetDecision\n",
      "schedule.TracePop\n",
      "schedule.TraceAsJSON\n",
      "transform.BF16Promote\n",
      "transform.BF16CastElimination\n",
      "transform.BF16TypeLowering\n",
      "transform.CompactBufferAllocation\n",
      "transform.ConvertForLoopsToSerial\n",
      "transform.DecorateDeviceScope\n",
      "transform.InjectCopyIntrin\n",
      "transform.HoistIfThenElseBasic\n",
      "transform.InjectPrefetch\n",
      "transform.LegalizePackedCalls\n",
      "schedule.ScheduleGetConsumers\n",
      "transform.LiftAttrScope\n",
      "transform.LowerCustomDatatypes\n",
      "transform.LowerMatchBuffer\n",
      "transform.PlanAndUpdateBufferAllocationLocation\n",
      "left_shift\n",
      "transform.LowerThreadAllreduce\n",
      "transform.LowerWarpMemory\n",
      "transform.MakeUnpackedAPI\n",
      "transform.MergeDynamicSharedMemoryAllocations\n",
      "transform.RemapThreadAxis\n",
      "transform.InjectDoubleBuffer\n",
      "transform.StorageRewrite\n",
      "bitwise_not\n",
      "transform.PointerValueTypeRewrite\n",
      "transform.ThreadSync\n",
      "transform.UnrollLoop\n",
      "transform.VectorizeLoop\n",
      "LayoutGetItem\n",
      "schedule.BlockScopeGetDepsByDst\n",
      "transform.MakePackedAPI\n",
      "_OpSub\n",
      "_OpLT\n",
      "Call\n",
      "schedule.ScheduleSamplePerfectTile\n",
      "schedule.TraceAsPython\n",
      "schedule.ScheduleSplit\n",
      "schedule.StmtSRefRootMark\n",
      "schedule.ScheduleState\n",
      "_OpMin\n",
      "schedule.InstructionKindGet\n",
      "ScheduleEnterPostproc\n",
      "TracedSchedule\n",
      "ScheduleGetBlock\n",
      "ScheduleBind\n",
      "ScheduleCopy\n",
      "ScheduleStorageAlign\n",
      "ScheduleComputeAt\n",
      "ScheduleGetLoops\n",
      "ScheduleReverseComputeAt\n",
      "ScheduleRFactor\n",
      "ScheduleStateReplace\n",
      "ScheduleDecomposeReduction\n",
      "TraceSimplified\n",
      "TraceWithDecision\n",
      "StmtSRefStmt\n",
      "TraceApplyJSONToSchedule\n",
      "ScheduleGetTrace\n",
      "ScheduleComputeInline\n",
      "ScheduleStateGetSRef\n",
      "StmtSRefParent\n",
      "LoopRV\n",
      "ConcreteSchedule\n",
      "TraceAppend\n",
      "ScheduleReverseComputeInline\n",
      "BlockRV\n",
      "RegisterReducer\n",
      "Instruction\n",
      "StmtSRefInlineMark\n",
      "BlockScopeGetDepsBySrc\n",
      "ScheduleGetMod\n",
      "ScheduleGetState\n",
      "ScheduleSeed\n",
      "ScheduleForkSeed\n",
      "ScheduleGet\n",
      "ScheduleGetSRef\n",
      "ScheduleRemoveRV\n",
      "ScheduleSampleCategorical\n",
      "ScheduleGetChildBlocks\n",
      "ScheduleGetProducers\n",
      "ScheduleFuse\n",
      "ScheduleReorder\n",
      "ScheduleParallel\n",
      "ScheduleVectorize\n",
      "ScheduleUnroll\n",
      "ScheduleCacheRead\n",
      "ScheduleCacheWrite\n",
      "ScheduleStateGetBlockScope\n",
      "ScheduleStateGetCachedFlags\n",
      "Trace\n",
      "TraceApplyToSchedule\n",
      "TraceGetDecision\n",
      "TracePop\n",
      "TraceAsJSON\n",
      "ScheduleGetConsumers\n",
      "BlockScopeGetDepsByDst\n",
      "ScheduleSamplePerfectTile\n",
      "TraceAsPython\n",
      "ScheduleSplit\n",
      "StmtSRefRootMark\n",
      "ScheduleState\n",
      "InstructionKindGet\n",
      "StorageFlatten\n",
      "BF16Legalize\n",
      "InferFragment\n",
      "CombineContextCall\n",
      "NarrowDataType\n",
      "RemoveNoOp\n",
      "SplitHostDevice\n",
      "LowerDeviceStorageAccessInfo\n",
      "UnifyThreadBinding\n",
      "InstrumentBoundCheckers\n",
      "LowerTVMBuiltin\n",
      "Simplify\n",
      "VerifyMemory\n",
      "VerifyGPUCode\n",
      "CreatePrimFuncPass\n",
      "InjectVirtualThread\n",
      "LowerInitBlock\n",
      "VerifySSA\n",
      "CoProcSync\n",
      "TextureFlatten\n",
      "RewriteUnsafeSelect\n",
      "ConvertBlocksToOpaque\n",
      "HoistIfThenElse\n",
      "FlattenBuffer\n",
      "SkipAssert\n",
      "LoopPartition\n",
      "LowerIntrin\n",
      "BF16Promote\n",
      "BF16CastElimination\n",
      "BF16TypeLowering\n",
      "CompactBufferAllocation\n",
      "ConvertForLoopsToSerial\n",
      "DecorateDeviceScope\n",
      "InjectCopyIntrin\n",
      "HoistIfThenElseBasic\n",
      "InjectPrefetch\n",
      "LegalizePackedCalls\n",
      "LiftAttrScope\n",
      "LowerCustomDatatypes\n",
      "LowerMatchBuffer\n",
      "PlanAndUpdateBufferAllocationLocation\n",
      "LowerThreadAllreduce\n",
      "LowerWarpMemory\n",
      "MakeUnpackedAPI\n",
      "MergeDynamicSharedMemoryAllocations\n",
      "RemapThreadAxis\n",
      "InjectDoubleBuffer\n",
      "StorageRewrite\n",
      "PointerValueTypeRewrite\n",
      "ThreadSync\n",
      "UnrollLoop\n",
      "VectorizeLoop\n",
      "MakePackedAPI\n",
      "detect_buffer_access_lca\n",
      "calculate_workspace_bytes\n",
      "GetBlockReadWriteRegion\n",
      "expr_deep_equal\n",
      "verify_gpu_code\n",
      "verify_memory\n",
      "verify_ssa\n",
      "GetBlockAccessRegion\n",
      "build.llvm\n",
      "build.nvptx\n",
      "build.rocm\n",
      "build.stackvm\n",
      "llvm_version_major\n",
      "MakeCompilationConfig\n",
      "build.sdaccel\n",
      "GenericFuncCallFunc\n",
      "Target\n",
      "TargetTagAddTag\n",
      "build.hexagon\n",
      "TargetExitScope\n",
      "TargetTagListTags\n",
      "build.opencl\n",
      "ListTargetKinds\n",
      "GenericFuncSetDefault\n",
      "build.metal\n",
      "GenericFuncCreate\n",
      "TargetCurrent\n",
      "llvm_lookup_intrinsic_id\n",
      "Build\n",
      "GenericFuncGetGlobal\n",
      "GenericFuncRegisterFunc\n",
      "SEScope_ForDeviceTargetAndMemoryScope\n",
      "build.aocl\n",
      "build.aocl_sw_emu\n",
      "build.c\n",
      "TargetEnterScope\n",
      "TargetExport\n",
      "WithHost\n",
      "ListTargetKindOptions\n",
      "OpInputTensors\n",
      "OpNumOutputs\n",
      "OpGetOutput\n",
      "GetCurrentSpecialization\n",
      "StageComputeInline\n",
      "ScheduleCacheRead\n",
      "ScheduleNormalize\n",
      "StagePragma\n",
      "StageUnroll\n",
      "StageSetStorePredicate\n",
      "StageFuse\n",
      "StageSplitByNParts\n",
      "t.op.MatchTensorizeBody\n",
      "StageReorder\n",
      "Gradient\n",
      "ting.FrontendTestModule\n",
      "ting.object_use_count\n",
      "ting.test_check_eq_callback\n",
      "ting.test_raise_error_callback\n",
      "ting.test_wrap_callback\n",
      "ting.echo\n",
      "ting.identity_cpp\n",
      "TensorComputeOp\n",
      "ScheduleCacheWrite\n",
      "StageDoubleBuffer\n",
      "CreateSchedule\n",
      "StageComputeRoot\n",
      "StagePrefetch\n",
      "ting.run_check_signal\n",
      "ting.ErrorTest\n",
      "Placeholder\n",
      "TensorIntrin\n",
      "StageBind\n",
      "StageVectorize\n",
      "HybridOp\n",
      "StageTensorize\n",
      "ExitSpecializationScope\n",
      "Tensor\n",
      "StageSetScope\n",
      "StageEnvThreads\n",
      "CreatePrimFuncFromOutputs\n",
      "CreateSpecializedCondition\n",
      "StageParallel\n",
      "StageComputeAt\n",
      "CreatePrimFunc\n",
      "t.op.InferTensorizeRegion\n",
      "StageStorageAlign\n",
      "EnterSpecializationScope\n",
      "TensorEqual\n",
      "ting.nop\n",
      "TensorIntrinCall\n",
      "ComputeOp\n",
      "ScheduleRFactor\n",
      "ExternOp\n",
      "TensorHash\n",
      "StageTile\n",
      "StageSplitByFactor\n",
      "ScanOp\n",
      "ting.device_test\n",
      "ScheduleCreateGroup\n",
      "PostDFSOrder\n",
      "ScanFixPointAnalysis\n",
      "ScanGetBody\n",
      "CreateAttachPath\n",
      "AutoInlineInjective\n",
      "AutoInlineElemWise\n",
      "InferBound\n",
      "SchedulePostProcToPrimFunc\n",
      "ScheduleOps\n",
      "AutoInlineBroadcast\n",
      "VerifyCompactBuffer\n",
      "CreateReadGraph\n",
      "DetectLinearEquation\n",
      "SolveLinearEquations\n",
      "IntConstraintsTransform\n",
      "IntSetIsEverything\n",
      "intset_vector\n",
      "PosInf\n",
      "EstimateRegionLowerBound\n",
      "NegInf\n",
      "IterMark\n",
      "IntervalSetGetMin\n",
      "DomainTouched\n",
      "IntervalSetGetMax\n",
      "CreateAnalyzer\n",
      "IntGroupBounds\n",
      "IntGroupBounds_FindBestRange\n",
      "IntSetIsNothing\n",
      "IterSumExpr\n",
      "intset_single_point\n",
      "IterSplitExpr\n",
      "SolveInequalitiesAsCondition\n",
      "ConstIntBound\n",
      "IntervalSet\n",
      "SolveInequalitiesToRange\n",
      "SubspaceDivide\n",
      "ModularSet\n",
      "InverseAffineIterMap\n",
      "IntConstraints\n",
      "DetectIterMap\n",
      "UnionLowerBound\n",
      "intset_interval\n",
      "DeduceBound\n",
      "IntGroupBounds_from_range\n",
      "SolveInequalitiesDeskewRange\n",
      "NormalizeIterMapToExpr\n",
      "DetectClipBound\n",
      "_Dump\n",
      "lower_primfunc\n",
      "preprocess_module\n",
      "mixed_mod_passes\n",
      "lower_module\n",
      "device_mod_passes\n",
      "host_mod_passes\n",
      "get_binds\n",
      "schedule_to_module\n",
      "lower_schedule\n",
      "SpanCheck\n",
      "ParseModule\n",
      "ParseModuleInContext\n",
      "ParseExpr\n",
      "GetLibInfo\n",
      "Function\n",
      "TempExprRealize\n",
      "RefWrite\n",
      "Call\n",
      "Tuple\n",
      "Clause\n",
      "PatternTuple\n",
      "PatternVar\n",
      "StaticMemoryPlan\n",
      "StorageInfoStorageSizes\n",
      "StorageInfo\n",
      "IsDynamic\n",
      "Bind\n",
      "cast\n",
      "StorageInfoStorageIds\n",
      "Constant\n",
      "PatternWildcard\n",
      "If\n",
      "Let\n",
      "Match\n",
      "StorageInfoDeviceTypes\n",
      "Var\n",
      "cast_like\n",
      "RefRead\n",
      "RefCreate\n",
      "PatternConstructor\n",
      "Any\n",
      "TupleGetItem\n",
      "MakeFunctionPass\n",
      "UnCPS\n",
      "ToCPS\n",
      "un_cps\n",
      "ToANormalForm\n",
      "Legalize\n",
      "LazyGradientInit\n",
      "Inline\n",
      "gradient\n",
      "FuseOps\n",
      "FoldExplicitPadding\n",
      "FoldConstantExpr\n",
      "FirstOrderGradient\n",
      "FakeQuantizationToInteger\n",
      "EliminateCommonSubexpr\n",
      "DefuseOps\n",
      "DenseToSparse\n",
      "Conv2dToSparse\n",
      "InferCorrectLayoutOutput\n",
      "ConvertLayout\n",
      "CombineParallelOpBatch\n",
      "CombineParallelConv2D\n",
      "AutoSchedulerLayoutRewrite\n",
      "AlterOpLayout\n",
      "ToANormalFormExpr\n",
      "PartialEvaluate\n",
      "FastMath\n",
      "MergeCompilerRegions\n",
      "CanonicalizeCast\n",
      "FoldScaleAxis\n",
      "SimplifyFCTranspose\n",
      "DynamicToStatic\n",
      "PlanDevices\n",
      "InferType\n",
      "SplitArgs\n",
      "CombineParallelDense\n",
      "ToBasicBlockNormalForm\n",
      "AnnotateSpans\n",
      "CanonicalizeOps\n",
      "CombineParallelBatchMatmul\n",
      "BackwardFoldScaleAxis\n",
      "InlinePrimitives\n",
      "EtaExpand\n",
      "dedup\n",
      "DeadCodeElimination\n",
      "PartitionGraph\n",
      "LambdaLift\n",
      "MergeComposite\n",
      "ToMixedPrecision\n",
      "RemoveUnusedFunctions\n",
      "SimplifyInference\n",
      "FoldConstant\n",
      "Defunctionalization\n",
      "Conv2dToSparse2\n",
      "ForwardFoldScaleAxis\n",
      "to_cps\n",
      "SimplifyExpr\n",
      "AnnotateTarget\n",
      "ToGraphNormalForm\n",
      "ServerLoop\n",
      "Connect\n",
      "CreatePipeClient\n",
      "CreateEventDrivenServer\n",
      "LocalSession\n",
      "LoadRemoteModule\n",
      "SessTableIndex\n",
      "ImportRemoteModule\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LowerToTE\n",
      "_TECompilerListItems\n",
      "_TECompilerJIT\n",
      "_TECompilerLower\n",
      "ListRuntimeOptions\n",
      "ListRuntimes\n",
      "GetRuntimeAttrs\n",
      "SanitizeName\n",
      "ToCConstantStyle\n",
      "ToCVariableStyle\n",
      "EvalFunction\n",
      "GraphPlanMemory\n",
      "ListExecutorOptions\n",
      "GetExecutorAttrs\n",
      "CreateExecutor\n",
      "_make_CCacheKey\n",
      "_make_LoweredOutput\n",
      "ToCFunctionStyle\n",
      "CreateRuntime\n",
      "ListExecutors\n",
      "PrefixGeneratedName\n",
      "_TECompilerGlobal\n",
      "_TECompilerClear\n",
      "build\n",
      "PrefixName\n",
      "_GraphExecutorCodegen\n",
      "_BuildModule\n",
      "_AOTExecutorCodegen\n",
      "BindParamsByName\n",
      "RefValue\n",
      "reinterpret\n",
      "ConstructorValue\n",
      "_VMCompiler\n",
      "post_order_visit\n",
      "search_fc_transpose\n",
      "check_constant\n",
      "all_vars\n",
      "free_vars\n",
      "_test_type_solver\n",
      "GetTotalMacNumber\n",
      "detect_feature\n",
      "ExtractOperators\n",
      "ExtractFusedFunctions\n",
      "CallGraph\n",
      "GetRegion\n",
      "bound_vars\n",
      "GetRefCountGlobalVar\n",
      "all_type_vars\n",
      "get_calibrate_module\n",
      "PrintCallGraph\n",
      "IsRecursive\n",
      "search_conv2d_op_weight\n",
      "get_calibrate_output_map\n",
      "PrintCallGraphGlobalVar\n",
      "GetModule\n",
      "search_dense_op_weight\n",
      "AnnotatedRegionSet\n",
      "check_kind\n",
      "GetGlobalVarCallCount\n",
      "well_formed\n",
      "unmatched_cases\n",
      "bound_type_vars\n",
      "check_basic_block_normal_form\n",
      "free_type_vars\n",
      "all_dtypes\n",
      "negative\n",
      "fast_tanh\n",
      "trunc\n",
      "floor\n",
      "fixed_point_multiply\n",
      "copy\n",
      "zeros_like\n",
      "sqrt\n",
      "erf\n",
      "exp\n",
      "atan\n",
      "asinh\n",
      "acos\n",
      "cos\n",
      "log2\n",
      "log\n",
      "invert_permutation\n",
      "cumprod\n",
      "adv_index\n",
      "unravel_index\n",
      "one_hot\n",
      "sequence_mask\n",
      "gather\n",
      "contrib_reverse_reshape\n",
      "auto_scheduler_layout_transform\n",
      "layout_transform\n",
      "broadcast_to\n",
      "reverse_sequence\n",
      "tile\n",
      "sparse_reshape\n",
      "sparse_fill_empty_rows\n",
      "ones\n",
      "zeros\n",
      "take\n",
      "scatter\n",
      "reshape_like\n",
      "reshape\n",
      "transpose\n",
      "stack\n",
      "concatenate\n",
      "expand_dims\n",
      "_variance\n",
      "mean\n",
      "prod\n",
      "min\n",
      "max\n",
      "any\n",
      "all\n",
      "sum\n",
      "argmin\n",
      "argmax\n",
      "einsum\n",
      "greater_equal\n",
      "greater\n",
      "less\n",
      "not_equal\n",
      "equal\n",
      "bitwise_xor\n",
      "logical_xor\n",
      "logical_or\n",
      "floor_mod\n",
      "floor_divide\n",
      "maximum\n",
      "right_shift\n",
      "subtract\n",
      "unique\n",
      "strided_slice\n",
      "multiply\n",
      "less_equal\n",
      "slice_like\n",
      "OpStrategy\n",
      "atanh\n",
      "device_copy\n",
      "tanh\n",
      "sign\n",
      "tan\n",
      "left_shift\n",
      "isfinite\n",
      "matrix_set_diag\n",
      "sin\n",
      "fast_erf\n",
      "isinf\n",
      "full_like\n",
      "isnan\n",
      "where\n",
      "ethosu_pooling\n",
      "logical_and\n",
      "arange\n",
      "rsqrt\n",
      "reverse\n",
      "meshgrid\n",
      "ones_like\n",
      "clip\n",
      "cumsum\n",
      "mod\n",
      "argwhere\n",
      "collapse_sum_like\n",
      "ceil\n",
      "ndarray_size\n",
      "minimum\n",
      "asin\n",
      "full\n",
      "gather_nd\n",
      "strided_set\n",
      "topk\n",
      "abs\n",
      "cosh\n",
      "scatter_nd\n",
      "squeeze\n",
      "divide\n",
      "power\n",
      "debug\n",
      "sinh\n",
      "searchsorted\n",
      "sparse_to_dense\n",
      "ethosu_conv2d\n",
      "broadcast_to_like\n",
      "round\n",
      "ethosu_depthwise_conv2d\n",
      "split\n",
      "collapse_sum_to\n",
      "add\n",
      "bitwise_or\n",
      "shape_of\n",
      "bitwise_not\n",
      "repeat\n",
      "scatter_add\n",
      "sigmoid\n",
      "fast_exp\n",
      "log10\n",
      "logical_not\n",
      "acosh\n",
      "argsort\n",
      "sort\n",
      "bitwise_and\n",
      "compiler._build\n",
      "compiler._lower\n",
      "_OpImplementationSchedule\n",
      "vm.reshape_tensor\n",
      "vm.invoke_tvm_op\n",
      "vm.shape_func\n",
      "vision._make.yolo_reorg\n",
      "vision._make.roi_pool\n",
      "vision._make.get_valid_counts\n",
      "vision._make.multibox_transform_loc\n",
      "vision._make.multibox_prior\n",
      "_make.negative\n",
      "_make.fast_tanh\n",
      "vm.shape_of\n",
      "_make.trunc\n",
      "_make.floor\n",
      "_make.fixed_point_multiply\n",
      "_make.copy\n",
      "_make.zeros_like\n",
      "_make.sqrt\n",
      "_make.erf\n",
      "_make.exp\n",
      "_make.atan\n",
      "_make.asinh\n",
      "_make.acos\n",
      "_make.cos\n",
      "_make.log2\n",
      "_make.log\n",
      "_make.invert_permutation\n",
      "_make.cumprod\n",
      "_make.adv_index\n",
      "_make.unravel_index\n",
      "_make.one_hot\n",
      "_make.sequence_mask\n",
      "_make.gather\n",
      "_make.contrib_reverse_reshape\n",
      "_make.auto_scheduler_layout_transform\n",
      "_make.layout_transform\n",
      "_make.broadcast_to\n",
      "_make.reverse_sequence\n",
      "_make.tile\n",
      "_make.sparse_reshape\n",
      "_make.sparse_fill_empty_rows\n",
      "_make.ones\n",
      "_make.zeros\n",
      "_make.take\n",
      "_make.scatter\n",
      "_make.reshape_like\n",
      "_make.reshape\n",
      "_make.transpose\n",
      "_make.stack\n",
      "_make.concatenate\n",
      "_make.expand_dims\n",
      "_make._variance\n",
      "_make.mean\n",
      "_make.prod\n",
      "_make.min\n",
      "_make.max\n",
      "_make.any\n",
      "_make.all\n",
      "_make.sum\n",
      "_make.argmin\n",
      "_make.argmax\n",
      "_make.einsum\n",
      "_make.greater_equal\n",
      "_make.greater\n",
      "_make.less\n",
      "_make.not_equal\n",
      "_make.equal\n",
      "_make.bitwise_xor\n",
      "_make.logical_xor\n",
      "_make.logical_or\n",
      "_make.floor_mod\n",
      "_make.floor_divide\n",
      "_make.maximum\n",
      "_make.right_shift\n",
      "_make.subtract\n",
      "vision._make.all_class_non_max_suppression\n",
      "random._make.threefry_split\n",
      "random._make.threefry_generate\n",
      "nn._make.upsampling3d\n",
      "nn._make.sparse_conv2d\n",
      "nn._make.sparse_add\n",
      "_make.unique\n",
      "nn._make.sparse_transpose\n",
      "nn._make.sparse_dense_padded\n",
      "_make.strided_slice\n",
      "nn._make.avg_pool3d\n",
      "_make.multiply\n",
      "nn._make.max_pool3d\n",
      "nn._make.avg_pool2d_grad\n",
      "nn._make.max_pool2d_grad\n",
      "nn._make.adaptive_avg_pool3d\n",
      "nn._make.adaptive_max_pool2d\n",
      "nn._make.adaptive_avg_pool2d\n",
      "nn._make.global_max_pool2d\n",
      "nn._make.max_pool2d\n",
      "nn._make.mirror_pad\n",
      "nn._make.pad\n",
      "_make.less_equal\n",
      "nn._make.batch_to_space_nd\n",
      "nn._make.space_to_depth\n",
      "nn._make.depth_to_space\n",
      "nn._make.nll_loss\n",
      "nn._make.cross_entropy_with_logits\n",
      "nn._make.dilate\n",
      "nn._make.cross_entropy\n",
      "nn._make.batch_matmul\n",
      "nn._make.group_norm\n",
      "nn._make.instance_norm\n",
      "nn._make.batch_norm\n",
      "nn._make.dropout\n",
      "nn._make.l2_normalize\n",
      "nn._make.lrn\n",
      "nn._make.relu\n",
      "nn._make.batch_flatten\n",
      "_make.slice_like\n",
      "nn._make.fast_softmax\n",
      "nn._make.softmax\n",
      "nn._make.prelu\n",
      "nn._make.leaky_relu\n",
      "nn._make.contrib_dense_pack\n",
      "nn._make.bias_add\n",
      "nn._make.contrib_conv2d_NCHWc\n",
      "nn._make.contrib_conv2d_gemm_weight_transform\n",
      "nn._make.contrib_conv2d_winograd_nnpack_weight_transform\n",
      "nn._make.conv2d_transpose\n",
      "nn._make.conv1d\n",
      "_make.OpStrategy\n",
      "nn._make.bitserial_dense\n",
      "nn._make.log_softmax\n",
      "memory._make.ToTupleType\n",
      "_make.atanh\n",
      "memory._make.FromTupleType\n",
      "memory._make.alloc_storage\n",
      "_make.device_copy\n",
      "image._make.crop_and_resize\n",
      "nn._make.contrib_conv3d_winograd_without_weight_transform\n",
      "image._make.resize3d\n",
      "image._make.resize1d\n",
      "image._make.grid_sample\n",
      "image._make.affine_grid\n",
      "dyn._make.sparse_to_dense\n",
      "dyn._make.strided_slice\n",
      "dyn._make.full\n",
      "dyn._make.one_hot\n",
      "_make.tanh\n",
      "dyn._make.zeros\n",
      "_make.sign\n",
      "_make.tan\n",
      "nn._make.contrib_depthwise_conv2d_NCHWc\n",
      "nn._make.adaptive_max_pool3d\n",
      "nn._make.adaptive_avg_pool1d\n",
      "annotation._make.stop_fusion\n",
      "_make.left_shift\n",
      "nn._make.bitserial_conv2d\n",
      "nn._make.deformable_conv2d\n",
      "_make.isfinite\n",
      "nn._make.avg_pool2d\n",
      "_make.matrix_set_diag\n",
      "_OpImplementationCompute\n",
      "_make.sin\n",
      "_make.fast_erf\n",
      "nn._make.bitpack\n",
      "nn._make.fifo_buffer\n",
      "nn._make.dense\n",
      "memory._make.FlattenTupleType\n",
      "_make.isinf\n",
      "_make.full_like\n",
      "image._make.resize2d\n",
      "_make.isnan\n",
      "_make.where\n",
      "_make.ethosu_pooling\n",
      "dyn._make.ones\n",
      "nn._make.space_to_batch_nd\n",
      "nn._make.upsampling\n",
      "_make.logical_and\n",
      "nn._make.correlation\n",
      "_make.arange\n",
      "vision._make.roi_align\n",
      "_make.rsqrt\n",
      "_make.reverse\n",
      "_make.meshgrid\n",
      "nn._make.contrib_conv2d_winograd_without_weight_transform\n",
      "nn._make.contrib_conv2d_winograd_weight_transform\n",
      "_make.ones_like\n",
      "random._make.uniform\n",
      "_make.clip\n",
      "_make.cumsum\n",
      "image._make.dilation2d\n",
      "dyn._make.squeeze\n",
      "dyn.nn._make.upsampling3d\n",
      "_make.mod\n",
      "_make.argwhere\n",
      "nn._make.contrib_conv3d_winograd_weight_transform\n",
      "dyn._make.expand_dims\n",
      "dyn._make.broadcast_to\n",
      "dyn.image._make.resize2d\n",
      "_make.collapse_sum_like\n",
      "_make.ceil\n",
      "annotation._make.checkpoint\n",
      "nn._make.conv3d_transpose\n",
      "_make.ndarray_size\n",
      "_make.minimum\n",
      "_make.asin\n",
      "_make.full\n",
      "_make.gather_nd\n",
      "_make.strided_set\n",
      "nn._make.matmul\n",
      "_make.topk\n",
      "_make.abs\n",
      "vision._make.non_max_suppression\n",
      "_make.cosh\n",
      "nn._make.layer_norm\n",
      "nn._make.adaptive_max_pool1d\n",
      "_make.scatter_nd\n",
      "nn._make.avg_pool1d\n",
      "_make.squeeze\n",
      "nn._make.conv3d\n",
      "_OpStrategyAddImplementation\n",
      "nn._make.contrib_conv2d_gemm_without_weight_transform\n",
      "annotation._make.on_device\n",
      "_make.divide\n",
      "_make.power\n",
      "annotation._make.compiler_end\n",
      "_make.debug\n",
      "nn._make.conv1d_transpose\n",
      "_make.sinh\n",
      "_make.searchsorted\n",
      "_make.sparse_to_dense\n",
      "_make.ethosu_conv2d\n",
      "_make.broadcast_to_like\n",
      "vision._make.proposal\n",
      "_make.round\n",
      "_make.ethosu_depthwise_conv2d\n",
      "_make.split\n",
      "nn._make.max_pool1d\n",
      "_make.collapse_sum_to\n",
      "dyn._make.topk\n",
      "nn._make.sparse_dense\n",
      "_make.add\n",
      "_make.bitwise_or\n",
      "_make.shape_of\n",
      "_make.bitwise_not\n",
      "nn._make.global_avg_pool2d\n",
      "annotation._make.function_on_device\n",
      "_make.repeat\n",
      "_make.scatter_add\n",
      "_make.sigmoid\n",
      "_make.fast_exp\n",
      "_make.log10\n",
      "memory._make.alloc_tensor\n",
      "_make.logical_not\n",
      "_make.acosh\n",
      "_make.argsort\n",
      "_make.sort\n",
      "nn._make.conv2d\n",
      "_make.bitwise_and\n",
      "annotation._make.compiler_begin\n",
      "dyn.nn._make.pad\n",
      "dyn.nn._make.upsampling\n",
      "dyn._make.reshape\n",
      "dyn._make.tile\n",
      "tensordot\n",
      "x86.default_schedule\n",
      "rocm.schedule_softmax\n",
      "exp\n",
      "x86.schedule_injective_from_existing\n",
      "nn.global_pool\n",
      "rsqrt\n",
      "greater_equal\n",
      "logical_or\n",
      "adv_index\n",
      "logical_xor\n",
      "nn.leaky_relu\n",
      "all\n",
      "mod\n",
      "subtract\n",
      "expand_dims\n",
      "asinh\n",
      "sequence_mask\n",
      "log10\n",
      "rocm.dense_cuda\n",
      "matrix_set_diag\n",
      "nn.pad\n",
      "atanh\n",
      "clip\n",
      "vision.reorg\n",
      "atan\n",
      "sum\n",
      "transpose\n",
      "fast_tanh\n",
      "max\n",
      "nn.batch_to_space_nd\n",
      "nn.space_to_batch_nd\n",
      "nn.adaptive_pool\n",
      "nn.binary_dense\n",
      "rocm.schedule_dense\n",
      "logical_and\n",
      "repeat\n",
      "nn.dense\n",
      "nn.pool1d\n",
      "cuda.schedule_softmax\n",
      "log\n",
      "cuda.dense_cuda\n",
      "unravel_index\n",
      "nn.pool2d\n",
      "acos\n",
      "nn.adaptive_pool3d\n",
      "acosh\n",
      "fast_exp\n",
      "cuda.schedule_injective\n",
      "nn.log_softmax\n",
      "nn.flatten\n",
      "negative\n",
      "add\n",
      "cos\n",
      "equal\n",
      "cuda.schedule_injective_from_existing\n",
      "less\n",
      "elemwise_sum\n",
      "rocm.schedule_reduce\n",
      "squeeze\n",
      "fast_erf\n",
      "nn.binarize_pack\n",
      "x86.schedule_binarize_pack\n",
      "x86.schedule_injective\n",
      "ndarray_size\n",
      "cuda.schedule_dense\n",
      "logical_not\n",
      "utils.bilinear_sample_nhwc\n",
      "floor_divide\n",
      "nn.dilate\n",
      "floor_mod\n",
      "maximum\n",
      "minimum\n",
      "power\n",
      "left_shift\n",
      "bitwise_and\n",
      "bitwise_or\n",
      "bitwise_xor\n",
      "right_shift\n",
      "full\n",
      "greater\n",
      "generic.schedule_injective_from_existing\n",
      "not_equal\n",
      "less_equal\n",
      "broadcast_to\n",
      "divide\n",
      "strided_slice\n",
      "asin\n",
      "tan\n",
      "cosh\n",
      "sin\n",
      "sinh\n",
      "tanh\n",
      "sigmoid\n",
      "sqrt\n",
      "identity\n",
      "cast\n",
      "reinterpret\n",
      "log2\n",
      "multiply\n",
      "sign\n",
      "bitwise_not\n",
      "nn.relu\n",
      "nn.prelu\n",
      "argmax\n",
      "nn.nll_loss\n",
      "nn.bias_add\n",
      "nn.scale_shift_nchw\n",
      "nn.pool_grad\n",
      "nn.pool3d\n",
      "nn.softmax\n",
      "nn.lrn\n",
      "min\n",
      "argmin\n",
      "gather\n",
      "prod\n",
      "any\n",
      "TEST_create_target\n",
      "generic.default_schedule\n",
      "generic.schedule_extern\n",
      "generic.schedule_injective\n",
      "rocm.schedule_injective\n",
      "split\n",
      "rocm.schedule_injective_from_existing\n",
      "rocm.schedule_pool\n",
      "rocm.schedule_global_pool\n",
      "cuda.schedule_pool\n",
      "cuda.schedule_global_pool\n",
      "full_like\n",
      "einsum\n",
      "cuda.schedule_reduce\n",
      "utils.is_empty_shape\n",
      "utils.bilinear_sample_nchw\n",
      "flip\n",
      "reverse_sequence\n",
      "nn.scale_shift_nhwc\n",
      "reshape\n",
      "erf\n",
      "tile\n",
      "concatenate\n",
      "stack\n",
      "shape\n",
      "layout_transform\n",
      "take\n",
      "where\n",
      "arange\n",
      "meshgrid\n",
      "gather_nd\n",
      "sparse_to_dense\n",
      "matmul\n",
      "dynamic_strided_slice\n",
      "one_hot\n",
      "x86.schedule_binary_dense\n",
      "schedule_softmax\n",
      "dense_cuda\n",
      "schedule_injective\n",
      "schedule_injective_from_existing\n",
      "schedule_dense\n",
      "schedule_pool\n",
      "schedule_global_pool\n",
      "schedule_reduce\n",
      "global_pool\n",
      "leaky_relu\n",
      "pad\n",
      "batch_to_space_nd\n",
      "space_to_batch_nd\n",
      "adaptive_pool\n",
      "binary_dense\n",
      "dense\n",
      "pool1d\n",
      "pool2d\n",
      "adaptive_pool3d\n",
      "log_softmax\n",
      "flatten\n",
      "binarize_pack\n",
      "dilate\n",
      "relu\n",
      "prelu\n",
      "nll_loss\n",
      "bias_add\n",
      "scale_shift_nchw\n",
      "pool_grad\n",
      "pool3d\n",
      "softmax\n",
      "lrn\n",
      "scale_shift_nhwc\n",
      "reorg\n",
      "default_schedule\n",
      "schedule_injective_from_existing\n",
      "schedule_binarize_pack\n",
      "schedule_injective\n",
      "schedule_binary_dense\n",
      "schedule_injective_from_existing\n",
      "default_schedule\n",
      "schedule_extern\n",
      "schedule_injective\n",
      "schedule_softmax\n",
      "dense_cuda\n",
      "schedule_dense\n",
      "schedule_reduce\n",
      "schedule_injective\n",
      "schedule_injective_from_existing\n",
      "schedule_pool\n",
      "schedule_global_pool\n",
      "bilinear_sample_nhwc\n",
      "is_empty_shape\n",
      "bilinear_sample_nchw\n",
      "SerializeSearchTask\n",
      "LocalRunner\n",
      "StateVectorize\n",
      "SaveRecords\n",
      "SketchPolicyEvolutionarySearch\n",
      "SearchPolicyUtilsGetConsumers\n",
      "SearchPolicyUtilsIsTiled\n",
      "DeserializeMeasureInput\n",
      "WriteMeasureRecords\n",
      "StatePragma\n",
      "PythonBasedMeasureCallback\n",
      "RPCRunner\n",
      "SearchPolicySetVerbose\n",
      "LocalBuilder\n",
      "StateUnroll\n",
      "PrintTitle\n",
      "MeasureInput\n",
      "StateEqual\n",
      "StateRfactor\n",
      "GetPerStoreFeaturesFromMeasurePairs\n",
      "StateComputeInline\n",
      "RecordReaderReadLines\n",
      "MeasureResult\n",
      "StateStorageAlign\n",
      "SketchPolicySampleInitialPopulation\n",
      "RecordReader\n",
      "StateFollowFusedSplit\n",
      "HardwareParams\n",
      "StateSplit\n",
      "StateReorder\n",
      "GetShapeFromRewrittenLayout\n",
      "BuildResult\n",
      "GetPerStoreFeatureNames\n",
      "SketchPolicy\n",
      "SketchPolicyGenerateSketches\n",
      "ProgramBuilderBuild\n",
      "GetDefaultHardwareParams\n",
      "CostModelUpdate\n",
      "StateBind\n",
      "StateCacheWrite\n",
      "StateComputeAt\n",
      "GetPerStoreFeaturesFromFile\n",
      "ProgramRunnerRun\n",
      "ComputeDAGPrintDAG\n",
      "StateParallel\n",
      "StateComputeRoot\n",
      "ReadMeasureRecord\n",
      "SearchPolicyUtilsIsElementwiseMatch\n",
      "RecordReaderReadNext\n",
      "PythonBasedModel\n",
      "ComputeDAG\n",
      "TuningOptions\n",
      "SearchPolicyContinueSearchOneRound\n",
      "DeserializeSearchTask\n",
      "StateFollowSplit\n",
      "ComputeDAGApplyStepsFromState\n",
      "SearchPolicyUtilsHasRfactorStage\n",
      "ComputeDAGInferBoundFromState\n",
      "EmptyPolicy\n",
      "ComputeDAGRewriteLayoutFromState\n",
      "SearchPolicyUtilsHasCacheWriteStage\n",
      "SearchPolicyRunCallbacks\n",
      "CostModelPredict\n",
      "ProgramMeasurer\n",
      "RecordToFile\n",
      "SearchPolicyUtilsHasCacheReadStage\n",
      "GetPerStoreFeaturesFromStates\n",
      "StateFuse\n",
      "RandomModel\n",
      "PreloadCustomSketchRule\n",
      "AutoSchedule\n",
      "SearchPolicyUtilsHasCrossThreadReduction\n",
      "ComputeDAGPrintPythonCodeFromState\n",
      "StateCacheRead\n",
      "PreloadMeasuredStates\n",
      "SearchTask\n",
      "RewriteIndexForNewLayout\n",
      "SerializeMeasureInput\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse_to_dense\n",
      "strided_slice\n",
      "full\n",
      "one_hot\n",
      "zeros\n",
      "ones\n",
      "squeeze\n",
      "expand_dims\n",
      "broadcast_to\n",
      "topk\n",
      "reshape\n",
      "tile\n",
      "reshape_tensor\n",
      "invoke_tvm_op\n",
      "shape_func\n",
      "shape_of\n",
      "upsampling3d\n",
      "pad\n",
      "upsampling\n",
      "upsampling3d\n",
      "sparse_conv2d\n",
      "sparse_add\n",
      "sparse_transpose\n",
      "sparse_dense_padded\n",
      "avg_pool3d\n",
      "max_pool3d\n",
      "avg_pool2d_grad\n",
      "max_pool2d_grad\n",
      "adaptive_avg_pool3d\n",
      "adaptive_max_pool2d\n",
      "adaptive_avg_pool2d\n",
      "global_max_pool2d\n",
      "max_pool2d\n",
      "mirror_pad\n",
      "pad\n",
      "batch_to_space_nd\n",
      "space_to_depth\n",
      "depth_to_space\n",
      "nll_loss\n",
      "cross_entropy_with_logits\n",
      "dilate\n",
      "cross_entropy\n",
      "batch_matmul\n",
      "group_norm\n",
      "instance_norm\n",
      "batch_norm\n",
      "dropout\n",
      "l2_normalize\n",
      "lrn\n",
      "relu\n",
      "batch_flatten\n",
      "fast_softmax\n",
      "softmax\n",
      "prelu\n",
      "leaky_relu\n",
      "contrib_dense_pack\n",
      "bias_add\n",
      "contrib_conv2d_NCHWc\n",
      "contrib_conv2d_gemm_weight_transform\n",
      "contrib_conv2d_winograd_nnpack_weight_transform\n",
      "conv2d_transpose\n",
      "conv1d\n",
      "bitserial_dense\n",
      "log_softmax\n",
      "contrib_conv3d_winograd_without_weight_transform\n",
      "contrib_depthwise_conv2d_NCHWc\n",
      "adaptive_max_pool3d\n",
      "adaptive_avg_pool1d\n",
      "bitserial_conv2d\n",
      "deformable_conv2d\n",
      "avg_pool2d\n",
      "bitpack\n",
      "fifo_buffer\n",
      "dense\n",
      "space_to_batch_nd\n",
      "upsampling\n",
      "correlation\n",
      "contrib_conv2d_winograd_without_weight_transform\n",
      "contrib_conv2d_winograd_weight_transform\n",
      "contrib_conv3d_winograd_weight_transform\n",
      "conv3d_transpose\n",
      "matmul\n",
      "layer_norm\n",
      "adaptive_max_pool1d\n",
      "avg_pool1d\n",
      "conv3d\n",
      "contrib_conv2d_gemm_without_weight_transform\n",
      "conv1d_transpose\n",
      "max_pool1d\n",
      "sparse_dense\n",
      "global_avg_pool2d\n",
      "conv2d\n",
      "stop_fusion\n",
      "checkpoint\n",
      "on_device\n",
      "compiler_end\n",
      "function_on_device\n",
      "compiler_begin\n",
      "ToTupleType\n",
      "FromTupleType\n",
      "alloc_storage\n",
      "FlattenTupleType\n",
      "alloc_tensor\n",
      "crop_and_resize\n",
      "resize3d\n",
      "resize1d\n",
      "grid_sample\n",
      "affine_grid\n",
      "resize2d\n",
      "dilation2d\n",
      "resize2d\n",
      "yolo_reorg\n",
      "roi_pool\n",
      "get_valid_counts\n",
      "multibox_transform_loc\n",
      "multibox_prior\n",
      "all_class_non_max_suppression\n",
      "roi_align\n",
      "non_max_suppression\n",
      "proposal\n",
      "threefry_split\n",
      "threefry_generate\n",
      "uniform\n",
      "DominatorPattern\n",
      "TypePattern\n",
      "TupleGetItemPattern\n",
      "TuplePattern\n",
      "LetPattern\n",
      "FunctionPattern\n",
      "ConstantPattern\n",
      "VarPattern\n",
      "rewrite\n",
      "DFPatternCallback\n",
      "partition\n",
      "CallPattern\n",
      "AttrPattern\n",
      "DataTypePattern\n",
      "WildcardPattern\n",
      "IfPattern\n",
      "match\n",
      "ShapePattern\n",
      "AltPattern\n",
      "ExprPattern\n",
      "quantize\n",
      "mul\n",
      "dequantize\n",
      "conv2d\n",
      "concatenate\n",
      "batch_matmul\n",
      "subtract\n",
      "add\n",
      "requantize\n",
      "conv2d_transpose\n",
      "dense\n",
      "simulated_quantize\n",
      "simulated_dequantize\n",
      "_EnterQConfigScope\n",
      "_GetCurrentQConfig\n",
      "simulated_quantize\n",
      "QuantizePartition\n",
      "QuantizeAnnotate\n",
      "make_annotate_expr\n",
      "make_partition_expr\n",
      "_ExitQConfigScope\n",
      "QuantizeRealize\n",
      "CreateStatsCollector\n",
      "FindScaleByKLMinimization\n",
      "input_1 (1, 3, 224, 224)\n",
      "-----mod[main]=fn (%input_1: Tensor[(1, 3, 224, 224), float32]) -> Tensor[(1, 1000), float32] {\n",
      "  %0 = nn.pad(%input_1, 0 /* ty=int32 */, pad_width=[[0, 0], [0, 0], [3, 3], [3, 3]]) /* ty=Tensor[(1, 3, 230, 230), float32] */;\n",
      "  %1 = nn.conv2d(%0, meta[relay.Constant][0] /* ty=Tensor[(64, 3, 7, 7), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=64, kernel_size=[7, 7]) /* ty=Tensor[(1, 64, 112, 112), float32] */;\n",
      "  %2 = nn.bias_add(%1, meta[relay.Constant][1] /* ty=Tensor[(64), float32] */) /* ty=Tensor[(1, 64, 112, 112), float32] */;\n",
      "  %3 = nn.batch_norm(%2, meta[relay.Constant][2] /* ty=Tensor[(64), float32] */, meta[relay.Constant][3] /* ty=Tensor[(64), float32] */, meta[relay.Constant][4] /* ty=Tensor[(64), float32] */, meta[relay.Constant][5] /* ty=Tensor[(64), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 64, 112, 112), float32], Tensor[(64), float32], Tensor[(64), float32]) */;\n",
      "  %4 = %3.0;\n",
      "  %5 = nn.relu(%4) /* ty=Tensor[(1, 64, 112, 112), float32] */;\n",
      "  %6 = nn.pad(%5, 0 /* ty=int32 */, pad_width=[[0, 0], [0, 0], [1, 1], [1, 1]]) /* ty=Tensor[(1, 64, 114, 114), float32] */;\n",
      "  %7 = nn.max_pool2d(%6, pool_size=[3, 3], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
      "  %8 = nn.conv2d(%7, meta[relay.Constant][6] /* ty=Tensor[(256, 64, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
      "  %9 = nn.bias_add(%8, meta[relay.Constant][7] /* ty=Tensor[(256), float32] */) /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
      "  %10 = nn.batch_norm(%9, meta[relay.Constant][8] /* ty=Tensor[(256), float32] */, meta[relay.Constant][9] /* ty=Tensor[(256), float32] */, meta[relay.Constant][10] /* ty=Tensor[(256), float32] */, meta[relay.Constant][11] /* ty=Tensor[(256), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 256, 56, 56), float32], Tensor[(256), float32], Tensor[(256), float32]) */;\n",
      "  %11 = nn.conv2d(%7, meta[relay.Constant][12] /* ty=Tensor[(64, 64, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
      "  %12 = nn.bias_add(%11, meta[relay.Constant][13] /* ty=Tensor[(64), float32] */) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
      "  %13 = nn.batch_norm(%12, meta[relay.Constant][14] /* ty=Tensor[(64), float32] */, meta[relay.Constant][15] /* ty=Tensor[(64), float32] */, meta[relay.Constant][16] /* ty=Tensor[(64), float32] */, meta[relay.Constant][17] /* ty=Tensor[(64), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;\n",
      "  %14 = %13.0;\n",
      "  %15 = nn.relu(%14) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
      "  %16 = nn.conv2d(%15, meta[relay.Constant][18] /* ty=Tensor[(64, 64, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
      "  %17 = nn.bias_add(%16, meta[relay.Constant][19] /* ty=Tensor[(64), float32] */) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
      "  %18 = nn.batch_norm(%17, meta[relay.Constant][20] /* ty=Tensor[(64), float32] */, meta[relay.Constant][21] /* ty=Tensor[(64), float32] */, meta[relay.Constant][22] /* ty=Tensor[(64), float32] */, meta[relay.Constant][23] /* ty=Tensor[(64), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;\n",
      "  %19 = %18.0;\n",
      "  %20 = nn.relu(%19) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
      "  %21 = nn.conv2d(%20, meta[relay.Constant][24] /* ty=Tensor[(256, 64, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
      "  %22 = nn.bias_add(%21, meta[relay.Constant][25] /* ty=Tensor[(256), float32] */) /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
      "  %23 = nn.batch_norm(%22, meta[relay.Constant][26] /* ty=Tensor[(256), float32] */, meta[relay.Constant][27] /* ty=Tensor[(256), float32] */, meta[relay.Constant][28] /* ty=Tensor[(256), float32] */, meta[relay.Constant][29] /* ty=Tensor[(256), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 256, 56, 56), float32], Tensor[(256), float32], Tensor[(256), float32]) */;\n",
      "  %24 = %10.0;\n",
      "  %25 = %23.0;\n",
      "  %26 = add(%24, %25) /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
      "  %27 = nn.relu(%26) /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
      "  %28 = nn.conv2d(%27, meta[relay.Constant][30] /* ty=Tensor[(64, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
      "  %29 = nn.bias_add(%28, meta[relay.Constant][31] /* ty=Tensor[(64), float32] */) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
      "  %30 = nn.batch_norm(%29, meta[relay.Constant][32] /* ty=Tensor[(64), float32] */, meta[relay.Constant][33] /* ty=Tensor[(64), float32] */, meta[relay.Constant][34] /* ty=Tensor[(64), float32] */, meta[relay.Constant][35] /* ty=Tensor[(64), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;\n",
      "  %31 = %30.0;\n",
      "  %32 = nn.relu(%31) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
      "  %33 = nn.conv2d(%32, meta[relay.Constant][36] /* ty=Tensor[(64, 64, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
      "  %34 = nn.bias_add(%33, meta[relay.Constant][37] /* ty=Tensor[(64), float32] */) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
      "  %35 = nn.batch_norm(%34, meta[relay.Constant][38] /* ty=Tensor[(64), float32] */, meta[relay.Constant][39] /* ty=Tensor[(64), float32] */, meta[relay.Constant][40] /* ty=Tensor[(64), float32] */, meta[relay.Constant][41] /* ty=Tensor[(64), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;\n",
      "  %36 = %35.0;\n",
      "  %37 = nn.relu(%36) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
      "  %38 = nn.conv2d(%37, meta[relay.Constant][42] /* ty=Tensor[(256, 64, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
      "  %39 = nn.bias_add(%38, meta[relay.Constant][43] /* ty=Tensor[(256), float32] */) /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
      "  %40 = nn.batch_norm(%39, meta[relay.Constant][44] /* ty=Tensor[(256), float32] */, meta[relay.Constant][45] /* ty=Tensor[(256), float32] */, meta[relay.Constant][46] /* ty=Tensor[(256), float32] */, meta[relay.Constant][47] /* ty=Tensor[(256), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 256, 56, 56), float32], Tensor[(256), float32], Tensor[(256), float32]) */;\n",
      "  %41 = %40.0;\n",
      "  %42 = add(%27, %41) /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
      "  %43 = nn.relu(%42) /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
      "  %44 = nn.conv2d(%43, meta[relay.Constant][48] /* ty=Tensor[(64, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
      "  %45 = nn.bias_add(%44, meta[relay.Constant][49] /* ty=Tensor[(64), float32] */) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
      "  %46 = nn.batch_norm(%45, meta[relay.Constant][50] /* ty=Tensor[(64), float32] */, meta[relay.Constant][51] /* ty=Tensor[(64), float32] */, meta[relay.Constant][52] /* ty=Tensor[(64), float32] */, meta[relay.Constant][53] /* ty=Tensor[(64), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;\n",
      "  %47 = %46.0;\n",
      "  %48 = nn.relu(%47) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
      "  %49 = nn.conv2d(%48, meta[relay.Constant][54] /* ty=Tensor[(64, 64, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
      "  %50 = nn.bias_add(%49, meta[relay.Constant][55] /* ty=Tensor[(64), float32] */) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
      "  %51 = nn.batch_norm(%50, meta[relay.Constant][56] /* ty=Tensor[(64), float32] */, meta[relay.Constant][57] /* ty=Tensor[(64), float32] */, meta[relay.Constant][58] /* ty=Tensor[(64), float32] */, meta[relay.Constant][59] /* ty=Tensor[(64), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;\n",
      "  %52 = %51.0;\n",
      "  %53 = nn.relu(%52) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
      "  %54 = nn.conv2d(%53, meta[relay.Constant][60] /* ty=Tensor[(256, 64, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
      "  %55 = nn.bias_add(%54, meta[relay.Constant][61] /* ty=Tensor[(256), float32] */) /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
      "  %56 = nn.batch_norm(%55, meta[relay.Constant][62] /* ty=Tensor[(256), float32] */, meta[relay.Constant][63] /* ty=Tensor[(256), float32] */, meta[relay.Constant][64] /* ty=Tensor[(256), float32] */, meta[relay.Constant][65] /* ty=Tensor[(256), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 256, 56, 56), float32], Tensor[(256), float32], Tensor[(256), float32]) */;\n",
      "  %57 = %56.0;\n",
      "  %58 = add(%43, %57) /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
      "  %59 = nn.relu(%58) /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
      "  %60 = nn.conv2d(%59, meta[relay.Constant][66] /* ty=Tensor[(512, 256, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
      "  %61 = nn.bias_add(%60, meta[relay.Constant][67] /* ty=Tensor[(512), float32] */) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
      "  %62 = nn.batch_norm(%61, meta[relay.Constant][68] /* ty=Tensor[(512), float32] */, meta[relay.Constant][69] /* ty=Tensor[(512), float32] */, meta[relay.Constant][70] /* ty=Tensor[(512), float32] */, meta[relay.Constant][71] /* ty=Tensor[(512), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 512, 28, 28), float32], Tensor[(512), float32], Tensor[(512), float32]) */;\n",
      "  %63 = nn.conv2d(%59, meta[relay.Constant][72] /* ty=Tensor[(128, 256, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
      "  %64 = nn.bias_add(%63, meta[relay.Constant][73] /* ty=Tensor[(128), float32] */) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
      "  %65 = nn.batch_norm(%64, meta[relay.Constant][74] /* ty=Tensor[(128), float32] */, meta[relay.Constant][75] /* ty=Tensor[(128), float32] */, meta[relay.Constant][76] /* ty=Tensor[(128), float32] */, meta[relay.Constant][77] /* ty=Tensor[(128), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;\n",
      "  %66 = %65.0;\n",
      "  %67 = nn.relu(%66) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
      "  %68 = nn.conv2d(%67, meta[relay.Constant][78] /* ty=Tensor[(128, 128, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
      "  %69 = nn.bias_add(%68, meta[relay.Constant][79] /* ty=Tensor[(128), float32] */) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
      "  %70 = nn.batch_norm(%69, meta[relay.Constant][80] /* ty=Tensor[(128), float32] */, meta[relay.Constant][81] /* ty=Tensor[(128), float32] */, meta[relay.Constant][82] /* ty=Tensor[(128), float32] */, meta[relay.Constant][83] /* ty=Tensor[(128), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;\n",
      "  %71 = %70.0;\n",
      "  %72 = nn.relu(%71) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
      "  %73 = nn.conv2d(%72, meta[relay.Constant][84] /* ty=Tensor[(512, 128, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
      "  %74 = nn.bias_add(%73, meta[relay.Constant][85] /* ty=Tensor[(512), float32] */) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
      "  %75 = nn.batch_norm(%74, meta[relay.Constant][86] /* ty=Tensor[(512), float32] */, meta[relay.Constant][87] /* ty=Tensor[(512), float32] */, meta[relay.Constant][88] /* ty=Tensor[(512), float32] */, meta[relay.Constant][89] /* ty=Tensor[(512), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 512, 28, 28), float32], Tensor[(512), float32], Tensor[(512), float32]) */;\n",
      "  %76 = %62.0;\n",
      "  %77 = %75.0;\n",
      "  %78 = add(%76, %77) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
      "  %79 = nn.relu(%78) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
      "  %80 = nn.conv2d(%79, meta[relay.Constant][90] /* ty=Tensor[(128, 512, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
      "  %81 = nn.bias_add(%80, meta[relay.Constant][91] /* ty=Tensor[(128), float32] */) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
      "  %82 = nn.batch_norm(%81, meta[relay.Constant][92] /* ty=Tensor[(128), float32] */, meta[relay.Constant][93] /* ty=Tensor[(128), float32] */, meta[relay.Constant][94] /* ty=Tensor[(128), float32] */, meta[relay.Constant][95] /* ty=Tensor[(128), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;\n",
      "  %83 = %82.0;\n",
      "  %84 = nn.relu(%83) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
      "  %85 = nn.conv2d(%84, meta[relay.Constant][96] /* ty=Tensor[(128, 128, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
      "  %86 = nn.bias_add(%85, meta[relay.Constant][97] /* ty=Tensor[(128), float32] */) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
      "  %87 = nn.batch_norm(%86, meta[relay.Constant][98] /* ty=Tensor[(128), float32] */, meta[relay.Constant][99] /* ty=Tensor[(128), float32] */, meta[relay.Constant][100] /* ty=Tensor[(128), float32] */, meta[relay.Constant][101] /* ty=Tensor[(128), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;\n",
      "  %88 = %87.0;\n",
      "  %89 = nn.relu(%88) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
      "  %90 = nn.conv2d(%89, meta[relay.Constant][102] /* ty=Tensor[(512, 128, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
      "  %91 = nn.bias_add(%90, meta[relay.Constant][103] /* ty=Tensor[(512), float32] */) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
      "  %92 = nn.batch_norm(%91, meta[relay.Constant][104] /* ty=Tensor[(512), float32] */, meta[relay.Constant][105] /* ty=Tensor[(512), float32] */, meta[relay.Constant][106] /* ty=Tensor[(512), float32] */, meta[relay.Constant][107] /* ty=Tensor[(512), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 512, 28, 28), float32], Tensor[(512), float32], Tensor[(512), float32]) */;\n",
      "  %93 = %92.0;\n",
      "  %94 = add(%79, %93) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
      "  %95 = nn.relu(%94) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
      "  %96 = nn.conv2d(%95, meta[relay.Constant][108] /* ty=Tensor[(128, 512, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
      "  %97 = nn.bias_add(%96, meta[relay.Constant][109] /* ty=Tensor[(128), float32] */) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
      "  %98 = nn.batch_norm(%97, meta[relay.Constant][110] /* ty=Tensor[(128), float32] */, meta[relay.Constant][111] /* ty=Tensor[(128), float32] */, meta[relay.Constant][112] /* ty=Tensor[(128), float32] */, meta[relay.Constant][113] /* ty=Tensor[(128), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;\n",
      "  %99 = %98.0;\n",
      "  %100 = nn.relu(%99) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
      "  %101 = nn.conv2d(%100, meta[relay.Constant][114] /* ty=Tensor[(128, 128, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
      "  %102 = nn.bias_add(%101, meta[relay.Constant][115] /* ty=Tensor[(128), float32] */) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
      "  %103 = nn.batch_norm(%102, meta[relay.Constant][116] /* ty=Tensor[(128), float32] */, meta[relay.Constant][117] /* ty=Tensor[(128), float32] */, meta[relay.Constant][118] /* ty=Tensor[(128), float32] */, meta[relay.Constant][119] /* ty=Tensor[(128), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;\n",
      "  %104 = %103.0;\n",
      "  %105 = nn.relu(%104) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
      "  %106 = nn.conv2d(%105, meta[relay.Constant][120] /* ty=Tensor[(512, 128, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
      "  %107 = nn.bias_add(%106, meta[relay.Constant][121] /* ty=Tensor[(512), float32] */) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
      "  %108 = nn.batch_norm(%107, meta[relay.Constant][122] /* ty=Tensor[(512), float32] */, meta[relay.Constant][123] /* ty=Tensor[(512), float32] */, meta[relay.Constant][124] /* ty=Tensor[(512), float32] */, meta[relay.Constant][125] /* ty=Tensor[(512), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 512, 28, 28), float32], Tensor[(512), float32], Tensor[(512), float32]) */;\n",
      "  %109 = %108.0;\n",
      "  %110 = add(%95, %109) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
      "  %111 = nn.relu(%110) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
      "  %112 = nn.conv2d(%111, meta[relay.Constant][126] /* ty=Tensor[(128, 512, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
      "  %113 = nn.bias_add(%112, meta[relay.Constant][127] /* ty=Tensor[(128), float32] */) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
      "  %114 = nn.batch_norm(%113, meta[relay.Constant][128] /* ty=Tensor[(128), float32] */, meta[relay.Constant][129] /* ty=Tensor[(128), float32] */, meta[relay.Constant][130] /* ty=Tensor[(128), float32] */, meta[relay.Constant][131] /* ty=Tensor[(128), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;\n",
      "  %115 = %114.0;\n",
      "  %116 = nn.relu(%115) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
      "  %117 = nn.conv2d(%116, meta[relay.Constant][132] /* ty=Tensor[(128, 128, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
      "  %118 = nn.bias_add(%117, meta[relay.Constant][133] /* ty=Tensor[(128), float32] */) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
      "  %119 = nn.batch_norm(%118, meta[relay.Constant][134] /* ty=Tensor[(128), float32] */, meta[relay.Constant][135] /* ty=Tensor[(128), float32] */, meta[relay.Constant][136] /* ty=Tensor[(128), float32] */, meta[relay.Constant][137] /* ty=Tensor[(128), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;\n",
      "  %120 = %119.0;\n",
      "  %121 = nn.relu(%120) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
      "  %122 = nn.conv2d(%121, meta[relay.Constant][138] /* ty=Tensor[(512, 128, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
      "  %123 = nn.bias_add(%122, meta[relay.Constant][139] /* ty=Tensor[(512), float32] */) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
      "  %124 = nn.batch_norm(%123, meta[relay.Constant][140] /* ty=Tensor[(512), float32] */, meta[relay.Constant][141] /* ty=Tensor[(512), float32] */, meta[relay.Constant][142] /* ty=Tensor[(512), float32] */, meta[relay.Constant][143] /* ty=Tensor[(512), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 512, 28, 28), float32], Tensor[(512), float32], Tensor[(512), float32]) */;\n",
      "  %125 = %124.0;\n",
      "  %126 = add(%111, %125) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
      "  %127 = nn.relu(%126) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
      "  %128 = nn.conv2d(%127, meta[relay.Constant][144] /* ty=Tensor[(1024, 512, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1]) /* ty=Tensor[(1, 1024, 14, 14), float32] */;\n",
      "  %129 = nn.bias_add(%128, meta[relay.Constant][145] /* ty=Tensor[(1024), float32] */) /* ty=Tensor[(1, 1024, 14, 14), float32] */;\n",
      "  %130 = nn.batch_norm(%129, meta[relay.Constant][146] /* ty=Tensor[(1024), float32] */, meta[relay.Constant][147] /* ty=Tensor[(1024), float32] */, meta[relay.Constant][148] /* ty=Tensor[(1024), float32] */, meta[relay.Constant][149] /* ty=Tensor[(1024), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 1024, 14, 14), float32], Tensor[(1024), float32], Tensor[(1024), float32]) */;\n",
      "  %131 = nn.conv2d(%127, meta[relay.Constant][150] /* ty=Tensor[(256, 512, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %132 = nn.bias_add(%131, meta[relay.Constant][151] /* ty=Tensor[(256), float32] */) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %133 = nn.batch_norm(%132, meta[relay.Constant][152] /* ty=Tensor[(256), float32] */, meta[relay.Constant][153] /* ty=Tensor[(256), float32] */, meta[relay.Constant][154] /* ty=Tensor[(256), float32] */, meta[relay.Constant][155] /* ty=Tensor[(256), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;\n",
      "  %134 = %133.0;\n",
      "  %135 = nn.relu(%134) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %136 = nn.conv2d(%135, meta[relay.Constant][156] /* ty=Tensor[(256, 256, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %137 = nn.bias_add(%136, meta[relay.Constant][157] /* ty=Tensor[(256), float32] */) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %138 = nn.batch_norm(%137, meta[relay.Constant][158] /* ty=Tensor[(256), float32] */, meta[relay.Constant][159] /* ty=Tensor[(256), float32] */, meta[relay.Constant][160] /* ty=Tensor[(256), float32] */, meta[relay.Constant][161] /* ty=Tensor[(256), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;\n",
      "  %139 = %138.0;\n",
      "  %140 = nn.relu(%139) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %141 = nn.conv2d(%140, meta[relay.Constant][162] /* ty=Tensor[(1024, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1]) /* ty=Tensor[(1, 1024, 14, 14), float32] */;\n",
      "  %142 = nn.bias_add(%141, meta[relay.Constant][163] /* ty=Tensor[(1024), float32] */) /* ty=Tensor[(1, 1024, 14, 14), float32] */;\n",
      "  %143 = nn.batch_norm(%142, meta[relay.Constant][164] /* ty=Tensor[(1024), float32] */, meta[relay.Constant][165] /* ty=Tensor[(1024), float32] */, meta[relay.Constant][166] /* ty=Tensor[(1024), float32] */, meta[relay.Constant][167] /* ty=Tensor[(1024), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 1024, 14, 14), float32], Tensor[(1024), float32], Tensor[(1024), float32]) */;\n",
      "  %144 = %130.0;\n",
      "  %145 = %143.0;\n",
      "  %146 = add(%144, %145) /* ty=Tensor[(1, 1024, 14, 14), float32] */;\n",
      "  %147 = nn.relu(%146) /* ty=Tensor[(1, 1024, 14, 14), float32] */;\n",
      "  %148 = nn.conv2d(%147, meta[relay.Constant][168] /* ty=Tensor[(256, 1024, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %149 = nn.bias_add(%148, meta[relay.Constant][169] /* ty=Tensor[(256), float32] */) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %150 = nn.batch_norm(%149, meta[relay.Constant][170] /* ty=Tensor[(256), float32] */, meta[relay.Constant][171] /* ty=Tensor[(256), float32] */, meta[relay.Constant][172] /* ty=Tensor[(256), float32] */, meta[relay.Constant][173] /* ty=Tensor[(256), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;\n",
      "  %151 = %150.0;\n",
      "  %152 = nn.relu(%151) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %153 = nn.conv2d(%152, meta[relay.Constant][174] /* ty=Tensor[(256, 256, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %154 = nn.bias_add(%153, meta[relay.Constant][175] /* ty=Tensor[(256), float32] */) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %155 = nn.batch_norm(%154, meta[relay.Constant][176] /* ty=Tensor[(256), float32] */, meta[relay.Constant][177] /* ty=Tensor[(256), float32] */, meta[relay.Constant][178] /* ty=Tensor[(256), float32] */, meta[relay.Constant][179] /* ty=Tensor[(256), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;\n",
      "  %156 = %155.0;\n",
      "  %157 = nn.relu(%156) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %158 = nn.conv2d(%157, meta[relay.Constant][180] /* ty=Tensor[(1024, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1]) /* ty=Tensor[(1, 1024, 14, 14), float32] */;\n",
      "  %159 = nn.bias_add(%158, meta[relay.Constant][181] /* ty=Tensor[(1024), float32] */) /* ty=Tensor[(1, 1024, 14, 14), float32] */;\n",
      "  %160 = nn.batch_norm(%159, meta[relay.Constant][182] /* ty=Tensor[(1024), float32] */, meta[relay.Constant][183] /* ty=Tensor[(1024), float32] */, meta[relay.Constant][184] /* ty=Tensor[(1024), float32] */, meta[relay.Constant][185] /* ty=Tensor[(1024), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 1024, 14, 14), float32], Tensor[(1024), float32], Tensor[(1024), float32]) */;\n",
      "  %161 = %160.0;\n",
      "  %162 = add(%147, %161) /* ty=Tensor[(1, 1024, 14, 14), float32] */;\n",
      "  %163 = nn.relu(%162) /* ty=Tensor[(1, 1024, 14, 14), float32] */;\n",
      "  %164 = nn.conv2d(%163, meta[relay.Constant][186] /* ty=Tensor[(256, 1024, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %165 = nn.bias_add(%164, meta[relay.Constant][187] /* ty=Tensor[(256), float32] */) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %166 = nn.batch_norm(%165, meta[relay.Constant][188] /* ty=Tensor[(256), float32] */, meta[relay.Constant][189] /* ty=Tensor[(256), float32] */, meta[relay.Constant][190] /* ty=Tensor[(256), float32] */, meta[relay.Constant][191] /* ty=Tensor[(256), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;\n",
      "  %167 = %166.0;\n",
      "  %168 = nn.relu(%167) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %169 = nn.conv2d(%168, meta[relay.Constant][192] /* ty=Tensor[(256, 256, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %170 = nn.bias_add(%169, meta[relay.Constant][193] /* ty=Tensor[(256), float32] */) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %171 = nn.batch_norm(%170, meta[relay.Constant][194] /* ty=Tensor[(256), float32] */, meta[relay.Constant][195] /* ty=Tensor[(256), float32] */, meta[relay.Constant][196] /* ty=Tensor[(256), float32] */, meta[relay.Constant][197] /* ty=Tensor[(256), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;\n",
      "  %172 = %171.0;\n",
      "  %173 = nn.relu(%172) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %174 = nn.conv2d(%173, meta[relay.Constant][198] /* ty=Tensor[(1024, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1]) /* ty=Tensor[(1, 1024, 14, 14), float32] */;\n",
      "  %175 = nn.bias_add(%174, meta[relay.Constant][199] /* ty=Tensor[(1024), float32] */) /* ty=Tensor[(1, 1024, 14, 14), float32] */;\n",
      "  %176 = nn.batch_norm(%175, meta[relay.Constant][200] /* ty=Tensor[(1024), float32] */, meta[relay.Constant][201] /* ty=Tensor[(1024), float32] */, meta[relay.Constant][202] /* ty=Tensor[(1024), float32] */, meta[relay.Constant][203] /* ty=Tensor[(1024), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 1024, 14, 14), float32], Tensor[(1024), float32], Tensor[(1024), float32]) */;\n",
      "  %177 = %176.0;\n",
      "  %178 = add(%163, %177) /* ty=Tensor[(1, 1024, 14, 14), float32] */;\n",
      "  %179 = nn.relu(%178) /* ty=Tensor[(1, 1024, 14, 14), float32] */;\n",
      "  %180 = nn.conv2d(%179, meta[relay.Constant][204] /* ty=Tensor[(256, 1024, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %181 = nn.bias_add(%180, meta[relay.Constant][205] /* ty=Tensor[(256), float32] */) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %182 = nn.batch_norm(%181, meta[relay.Constant][206] /* ty=Tensor[(256), float32] */, meta[relay.Constant][207] /* ty=Tensor[(256), float32] */, meta[relay.Constant][208] /* ty=Tensor[(256), float32] */, meta[relay.Constant][209] /* ty=Tensor[(256), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;\n",
      "  %183 = %182.0;\n",
      "  %184 = nn.relu(%183) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %185 = nn.conv2d(%184, meta[relay.Constant][210] /* ty=Tensor[(256, 256, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %186 = nn.bias_add(%185, meta[relay.Constant][211] /* ty=Tensor[(256), float32] */) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %187 = nn.batch_norm(%186, meta[relay.Constant][212] /* ty=Tensor[(256), float32] */, meta[relay.Constant][213] /* ty=Tensor[(256), float32] */, meta[relay.Constant][214] /* ty=Tensor[(256), float32] */, meta[relay.Constant][215] /* ty=Tensor[(256), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;\n",
      "  %188 = %187.0;\n",
      "  %189 = nn.relu(%188) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %190 = nn.conv2d(%189, meta[relay.Constant][216] /* ty=Tensor[(1024, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1]) /* ty=Tensor[(1, 1024, 14, 14), float32] */;\n",
      "  %191 = nn.bias_add(%190, meta[relay.Constant][217] /* ty=Tensor[(1024), float32] */) /* ty=Tensor[(1, 1024, 14, 14), float32] */;\n",
      "  %192 = nn.batch_norm(%191, meta[relay.Constant][218] /* ty=Tensor[(1024), float32] */, meta[relay.Constant][219] /* ty=Tensor[(1024), float32] */, meta[relay.Constant][220] /* ty=Tensor[(1024), float32] */, meta[relay.Constant][221] /* ty=Tensor[(1024), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 1024, 14, 14), float32], Tensor[(1024), float32], Tensor[(1024), float32]) */;\n",
      "  %193 = %192.0;\n",
      "  %194 = add(%179, %193) /* ty=Tensor[(1, 1024, 14, 14), float32] */;\n",
      "  %195 = nn.relu(%194) /* ty=Tensor[(1, 1024, 14, 14), float32] */;\n",
      "  %196 = nn.conv2d(%195, meta[relay.Constant][222] /* ty=Tensor[(256, 1024, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %197 = nn.bias_add(%196, meta[relay.Constant][223] /* ty=Tensor[(256), float32] */) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %198 = nn.batch_norm(%197, meta[relay.Constant][224] /* ty=Tensor[(256), float32] */, meta[relay.Constant][225] /* ty=Tensor[(256), float32] */, meta[relay.Constant][226] /* ty=Tensor[(256), float32] */, meta[relay.Constant][227] /* ty=Tensor[(256), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;\n",
      "  %199 = %198.0;\n",
      "  %200 = nn.relu(%199) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %201 = nn.conv2d(%200, meta[relay.Constant][228] /* ty=Tensor[(256, 256, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %202 = nn.bias_add(%201, meta[relay.Constant][229] /* ty=Tensor[(256), float32] */) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %203 = nn.batch_norm(%202, meta[relay.Constant][230] /* ty=Tensor[(256), float32] */, meta[relay.Constant][231] /* ty=Tensor[(256), float32] */, meta[relay.Constant][232] /* ty=Tensor[(256), float32] */, meta[relay.Constant][233] /* ty=Tensor[(256), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;\n",
      "  %204 = %203.0;\n",
      "  %205 = nn.relu(%204) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %206 = nn.conv2d(%205, meta[relay.Constant][234] /* ty=Tensor[(1024, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1]) /* ty=Tensor[(1, 1024, 14, 14), float32] */;\n",
      "  %207 = nn.bias_add(%206, meta[relay.Constant][235] /* ty=Tensor[(1024), float32] */) /* ty=Tensor[(1, 1024, 14, 14), float32] */;\n",
      "  %208 = nn.batch_norm(%207, meta[relay.Constant][236] /* ty=Tensor[(1024), float32] */, meta[relay.Constant][237] /* ty=Tensor[(1024), float32] */, meta[relay.Constant][238] /* ty=Tensor[(1024), float32] */, meta[relay.Constant][239] /* ty=Tensor[(1024), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 1024, 14, 14), float32], Tensor[(1024), float32], Tensor[(1024), float32]) */;\n",
      "  %209 = %208.0;\n",
      "  %210 = add(%195, %209) /* ty=Tensor[(1, 1024, 14, 14), float32] */;\n",
      "  %211 = nn.relu(%210) /* ty=Tensor[(1, 1024, 14, 14), float32] */;\n",
      "  %212 = nn.conv2d(%211, meta[relay.Constant][240] /* ty=Tensor[(256, 1024, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %213 = nn.bias_add(%212, meta[relay.Constant][241] /* ty=Tensor[(256), float32] */) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %214 = nn.batch_norm(%213, meta[relay.Constant][242] /* ty=Tensor[(256), float32] */, meta[relay.Constant][243] /* ty=Tensor[(256), float32] */, meta[relay.Constant][244] /* ty=Tensor[(256), float32] */, meta[relay.Constant][245] /* ty=Tensor[(256), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;\n",
      "  %215 = %214.0;\n",
      "  %216 = nn.relu(%215) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %217 = nn.conv2d(%216, meta[relay.Constant][246] /* ty=Tensor[(256, 256, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %218 = nn.bias_add(%217, meta[relay.Constant][247] /* ty=Tensor[(256), float32] */) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %219 = nn.batch_norm(%218, meta[relay.Constant][248] /* ty=Tensor[(256), float32] */, meta[relay.Constant][249] /* ty=Tensor[(256), float32] */, meta[relay.Constant][250] /* ty=Tensor[(256), float32] */, meta[relay.Constant][251] /* ty=Tensor[(256), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;\n",
      "  %220 = %219.0;\n",
      "  %221 = nn.relu(%220) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %222 = nn.conv2d(%221, meta[relay.Constant][252] /* ty=Tensor[(1024, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1]) /* ty=Tensor[(1, 1024, 14, 14), float32] */;\n",
      "  %223 = nn.bias_add(%222, meta[relay.Constant][253] /* ty=Tensor[(1024), float32] */) /* ty=Tensor[(1, 1024, 14, 14), float32] */;\n",
      "  %224 = nn.batch_norm(%223, meta[relay.Constant][254] /* ty=Tensor[(1024), float32] */, meta[relay.Constant][255] /* ty=Tensor[(1024), float32] */, meta[relay.Constant][256] /* ty=Tensor[(1024), float32] */, meta[relay.Constant][257] /* ty=Tensor[(1024), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 1024, 14, 14), float32], Tensor[(1024), float32], Tensor[(1024), float32]) */;\n",
      "  %225 = %224.0;\n",
      "  %226 = add(%211, %225) /* ty=Tensor[(1, 1024, 14, 14), float32] */;\n",
      "  %227 = nn.relu(%226) /* ty=Tensor[(1, 1024, 14, 14), float32] */;\n",
      "  %228 = nn.conv2d(%227, meta[relay.Constant][258] /* ty=Tensor[(2048, 1024, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=2048, kernel_size=[1, 1]) /* ty=Tensor[(1, 2048, 7, 7), float32] */;\n",
      "  %229 = nn.bias_add(%228, meta[relay.Constant][259] /* ty=Tensor[(2048), float32] */) /* ty=Tensor[(1, 2048, 7, 7), float32] */;\n",
      "  %230 = nn.batch_norm(%229, meta[relay.Constant][260] /* ty=Tensor[(2048), float32] */, meta[relay.Constant][261] /* ty=Tensor[(2048), float32] */, meta[relay.Constant][262] /* ty=Tensor[(2048), float32] */, meta[relay.Constant][263] /* ty=Tensor[(2048), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 2048, 7, 7), float32], Tensor[(2048), float32], Tensor[(2048), float32]) */;\n",
      "  %231 = nn.conv2d(%227, meta[relay.Constant][264] /* ty=Tensor[(512, 1024, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
      "  %232 = nn.bias_add(%231, meta[relay.Constant][265] /* ty=Tensor[(512), float32] */) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
      "  %233 = nn.batch_norm(%232, meta[relay.Constant][266] /* ty=Tensor[(512), float32] */, meta[relay.Constant][267] /* ty=Tensor[(512), float32] */, meta[relay.Constant][268] /* ty=Tensor[(512), float32] */, meta[relay.Constant][269] /* ty=Tensor[(512), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;\n",
      "  %234 = %233.0;\n",
      "  %235 = nn.relu(%234) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
      "  %236 = nn.conv2d(%235, meta[relay.Constant][270] /* ty=Tensor[(512, 512, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
      "  %237 = nn.bias_add(%236, meta[relay.Constant][271] /* ty=Tensor[(512), float32] */) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
      "  %238 = nn.batch_norm(%237, meta[relay.Constant][272] /* ty=Tensor[(512), float32] */, meta[relay.Constant][273] /* ty=Tensor[(512), float32] */, meta[relay.Constant][274] /* ty=Tensor[(512), float32] */, meta[relay.Constant][275] /* ty=Tensor[(512), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;\n",
      "  %239 = %238.0;\n",
      "  %240 = nn.relu(%239) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
      "  %241 = nn.conv2d(%240, meta[relay.Constant][276] /* ty=Tensor[(2048, 512, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=2048, kernel_size=[1, 1]) /* ty=Tensor[(1, 2048, 7, 7), float32] */;\n",
      "  %242 = nn.bias_add(%241, meta[relay.Constant][277] /* ty=Tensor[(2048), float32] */) /* ty=Tensor[(1, 2048, 7, 7), float32] */;\n",
      "  %243 = nn.batch_norm(%242, meta[relay.Constant][278] /* ty=Tensor[(2048), float32] */, meta[relay.Constant][279] /* ty=Tensor[(2048), float32] */, meta[relay.Constant][280] /* ty=Tensor[(2048), float32] */, meta[relay.Constant][281] /* ty=Tensor[(2048), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 2048, 7, 7), float32], Tensor[(2048), float32], Tensor[(2048), float32]) */;\n",
      "  %244 = %230.0;\n",
      "  %245 = %243.0;\n",
      "  %246 = add(%244, %245) /* ty=Tensor[(1, 2048, 7, 7), float32] */;\n",
      "  %247 = nn.relu(%246) /* ty=Tensor[(1, 2048, 7, 7), float32] */;\n",
      "  %248 = nn.conv2d(%247, meta[relay.Constant][282] /* ty=Tensor[(512, 2048, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
      "  %249 = nn.bias_add(%248, meta[relay.Constant][283] /* ty=Tensor[(512), float32] */) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
      "  %250 = nn.batch_norm(%249, meta[relay.Constant][284] /* ty=Tensor[(512), float32] */, meta[relay.Constant][285] /* ty=Tensor[(512), float32] */, meta[relay.Constant][286] /* ty=Tensor[(512), float32] */, meta[relay.Constant][287] /* ty=Tensor[(512), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;\n",
      "  %251 = %250.0;\n",
      "  %252 = nn.relu(%251) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
      "  %253 = nn.conv2d(%252, meta[relay.Constant][288] /* ty=Tensor[(512, 512, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
      "  %254 = nn.bias_add(%253, meta[relay.Constant][289] /* ty=Tensor[(512), float32] */) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
      "  %255 = nn.batch_norm(%254, meta[relay.Constant][290] /* ty=Tensor[(512), float32] */, meta[relay.Constant][291] /* ty=Tensor[(512), float32] */, meta[relay.Constant][292] /* ty=Tensor[(512), float32] */, meta[relay.Constant][293] /* ty=Tensor[(512), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;\n",
      "  %256 = %255.0;\n",
      "  %257 = nn.relu(%256) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
      "  %258 = nn.conv2d(%257, meta[relay.Constant][294] /* ty=Tensor[(2048, 512, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=2048, kernel_size=[1, 1]) /* ty=Tensor[(1, 2048, 7, 7), float32] */;\n",
      "  %259 = nn.bias_add(%258, meta[relay.Constant][295] /* ty=Tensor[(2048), float32] */) /* ty=Tensor[(1, 2048, 7, 7), float32] */;\n",
      "  %260 = nn.batch_norm(%259, meta[relay.Constant][296] /* ty=Tensor[(2048), float32] */, meta[relay.Constant][297] /* ty=Tensor[(2048), float32] */, meta[relay.Constant][298] /* ty=Tensor[(2048), float32] */, meta[relay.Constant][299] /* ty=Tensor[(2048), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 2048, 7, 7), float32], Tensor[(2048), float32], Tensor[(2048), float32]) */;\n",
      "  %261 = %260.0;\n",
      "  %262 = add(%247, %261) /* ty=Tensor[(1, 2048, 7, 7), float32] */;\n",
      "  %263 = nn.relu(%262) /* ty=Tensor[(1, 2048, 7, 7), float32] */;\n",
      "  %264 = nn.conv2d(%263, meta[relay.Constant][300] /* ty=Tensor[(512, 2048, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
      "  %265 = nn.bias_add(%264, meta[relay.Constant][301] /* ty=Tensor[(512), float32] */) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
      "  %266 = nn.batch_norm(%265, meta[relay.Constant][302] /* ty=Tensor[(512), float32] */, meta[relay.Constant][303] /* ty=Tensor[(512), float32] */, meta[relay.Constant][304] /* ty=Tensor[(512), float32] */, meta[relay.Constant][305] /* ty=Tensor[(512), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;\n",
      "  %267 = %266.0;\n",
      "  %268 = nn.relu(%267) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
      "  %269 = nn.conv2d(%268, meta[relay.Constant][306] /* ty=Tensor[(512, 512, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
      "  %270 = nn.bias_add(%269, meta[relay.Constant][307] /* ty=Tensor[(512), float32] */) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
      "  %271 = nn.batch_norm(%270, meta[relay.Constant][308] /* ty=Tensor[(512), float32] */, meta[relay.Constant][309] /* ty=Tensor[(512), float32] */, meta[relay.Constant][310] /* ty=Tensor[(512), float32] */, meta[relay.Constant][311] /* ty=Tensor[(512), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;\n",
      "  %272 = %271.0;\n",
      "  %273 = nn.relu(%272) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
      "  %274 = nn.conv2d(%273, meta[relay.Constant][312] /* ty=Tensor[(2048, 512, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=2048, kernel_size=[1, 1]) /* ty=Tensor[(1, 2048, 7, 7), float32] */;\n",
      "  %275 = nn.bias_add(%274, meta[relay.Constant][313] /* ty=Tensor[(2048), float32] */) /* ty=Tensor[(1, 2048, 7, 7), float32] */;\n",
      "  %276 = nn.batch_norm(%275, meta[relay.Constant][314] /* ty=Tensor[(2048), float32] */, meta[relay.Constant][315] /* ty=Tensor[(2048), float32] */, meta[relay.Constant][316] /* ty=Tensor[(2048), float32] */, meta[relay.Constant][317] /* ty=Tensor[(2048), float32] */, epsilon=1.001e-05f) /* ty=(Tensor[(1, 2048, 7, 7), float32], Tensor[(2048), float32], Tensor[(2048), float32]) */;\n",
      "  %277 = %276.0;\n",
      "  %278 = add(%263, %277) /* ty=Tensor[(1, 2048, 7, 7), float32] */;\n",
      "  %279 = nn.relu(%278) /* ty=Tensor[(1, 2048, 7, 7), float32] */;\n",
      "  %280 = nn.global_avg_pool2d(%279) /* ty=Tensor[(1, 2048, 1, 1), float32] */;\n",
      "  %281 = transpose(%280, axes=[0, 2, 3, 1]) /* ty=Tensor[(1, 1, 1, 2048), float32] */;\n",
      "  %282 = nn.batch_flatten(%281) /* ty=Tensor[(1, 2048), float32] */;\n",
      "  %283 = nn.dense(%282, meta[relay.Constant][318] /* ty=Tensor[(1000, 2048), float32] */, units=1000) /* ty=Tensor[(1, 1000), float32] */;\n",
      "  %284 = nn.bias_add(%283, meta[relay.Constant][319] /* ty=Tensor[(1000), float32] */) /* ty=Tensor[(1, 1000), float32] */;\n",
      "  nn.softmax(%284, axis=1) /* ty=Tensor[(1, 1000), float32] */\n",
      "}\n",
      "\n",
      "start to build\n",
      "Retrieve the executor from the target\n",
      "1\n",
      "with tophub_context:<tvm.autotvm.task.dispatcher.ApplyHistoryBest object at 0x7f9aa4a06dc0>\n",
      "target={1: llvm -keys=cpu -link-params=0}\n",
      "target={1: llvm -keys=cpu -link-params=0}, target_host=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--executor is graph--\n",
      "----mod[default]=Module(GraphExecutor, 555c2d338578)\n",
      "Module(GraphExecutor, 555c2d337ff8)\n",
      "<function GraphExecutor._make_executor.<locals>._graph_wrapper at 0x7f9aa49c2d30>\n",
      "-------------run--------------\n"
     ]
    }
   ],
   "source": [
    "import tvm\n",
    "from tvm import te\n",
    "import tvm.relay as relay\n",
    "from tvm.contrib.download import download_testdata\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "if tuple(keras.__version__.split(\".\")) < (\"2\", \"4\", \"0\"):\n",
    "    weights_url = \"\".join(\n",
    "        [\n",
    "            \"https://github.com/fchollet/deep-learning-models/releases/\",\n",
    "            \"download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels.h5\",\n",
    "        ]\n",
    "    )\n",
    "    weights_file = \"resnet50_keras_old.h5\"\n",
    "else:\n",
    "    weights_url = \"\".join(\n",
    "        [\n",
    "            \" https://storage.googleapis.com/tensorflow/keras-applications/\",\n",
    "            \"resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5\",\n",
    "        ]\n",
    "    )\n",
    "    weights_file = \"resnet50_keras_new.h5\"\n",
    "\n",
    "weights_path = download_testdata(weights_url, weights_file, module=\"keras\")\n",
    "keras_resnet50 = keras.applications.resnet50.ResNet50(\n",
    "    include_top=True, weights=None, input_shape=(224, 224, 3), classes=1000\n",
    ")                                       \n",
    "keras_resnet50.load_weights(weights_path)\n",
    "\n",
    "img_url = \"https://github.com/dmlc/mxnet.js/blob/main/data/cat.png?raw=true\"\n",
    "img_path = download_testdata(img_url, \"cat.png\", module=\"data\")\n",
    "img = Image.open(img_path).resize((224, 224))\n",
    "#plt.imshow(img)\n",
    "#plt.show()\n",
    "# input preprocess\n",
    "data = np.array(img)[np.newaxis, :].astype(\"float32\")\n",
    "data = preprocess_input(data).transpose([0, 3, 1, 2])   #NHWC -> NCHW   \n",
    "print(\"input_1\", data.shape)    \n",
    "shape_dict = {\"input_1\": data.shape}\n",
    "mod, params = relay.frontend.from_keras(keras_resnet50, shape_dict)\n",
    "# compile the model\n",
    "target = \"llvm\"\n",
    "dev = tvm.cpu(0)\n",
    "with tvm.transform.PassContext(opt_level=0):\n",
    "    model = relay.build_module.create_executor(\"graph\", mod, dev, target, params).evaluate()\n",
    "print(model)\n",
    "dtype = \"float32\"\n",
    "#tvm_out = model(tvm.nd.array(data.astype(dtype)))\n",
    "#tvm_out = tvm_out.numpy()[0]\n",
    "#top1_tvm = np.argmax(tvm_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3fc7142",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'top1_tvm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-6c40c3717be9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msynset_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0msynset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Relay top-1 id: {}, prob: {}, class name: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop1_tvm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtvm_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtop1_tvm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtop1_tvm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;31m# confirm correctness with keras output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mkeras_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras_resnet50\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'top1_tvm' is not defined"
     ]
    }
   ],
   "source": [
    "synset_url = \"\".join(\n",
    "    [\n",
    "        \"https://gist.githubusercontent.com/zhreshold/\",\n",
    "        \"4d0b62f3d01426887599d4f7ede23ee5/raw/\",\n",
    "        \"596b27d23537e5a1b5751d2b0481ef172f58b539/\",\n",
    "        \"imagenet1000_clsid_to_human.txt\",\n",
    "    ]\n",
    ")\n",
    "synset_name = \"imagenet1000_clsid_to_human.txt\"\n",
    "synset_path = download_testdata(synset_url, synset_name, module=\"data\")\n",
    "with open(synset_path) as f:\n",
    "    synset = eval(f.read())\n",
    "print(\"Relay top-1 id: {}, prob: {}, class name: {}\".format(top1_tvm, tvm_out[top1_tvm], synset[top1_tvm]))\n",
    "# confirm correctness with keras output\n",
    "keras_out = keras_resnet50.predict(data.transpose([0, 2, 3, 1]))\n",
    "keras_out = keras_out[0]\n",
    "top1_keras = np.argmax(keras_out)\n",
    "print(\"Keras top-1 id: {}, prob: {}, class name: {}\".format(top1_keras, keras_out[top1_keras], synset[top1_keras]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7951c5b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
