{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6f55c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 10s 6ms/step - loss: 0.5292 - accuracy: 0.8556 - val_loss: 0.1338 - val_accuracy: 0.9642\n",
      "WARNING:tensorflow:AutoGraph could not transform <tensorflow_model_optimization.python.core.quantization.keras.quantizers.AllValuesQuantizer object at 0x7ff4f29f8a30> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <tensorflow_model_optimization.python.core.quantization.keras.quantizers.AllValuesQuantizer object at 0x7ff4f29f8a30> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <tensorflow_model_optimization.python.core.quantization.keras.quantizers.AllValuesQuantizer object at 0x7ff4f29f8a30> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <tensorflow_model_optimization.python.core.quantization.keras.quantizers.AllValuesQuantizer object at 0x7ff4f29f8a30> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "quantize_layer_2 (QuantizeLa (None, 28, 28)            3         \n",
      "_________________________________________________________________\n",
      "quant_reshape_2 (QuantizeWra (None, 28, 28, 1)         1         \n",
      "_________________________________________________________________\n",
      "quant_conv2d_2 (QuantizeWrap (None, 26, 26, 12)        147       \n",
      "_________________________________________________________________\n",
      "quant_max_pooling2d_2 (Quant (None, 13, 13, 12)        1         \n",
      "_________________________________________________________________\n",
      "quant_flatten_2 (QuantizeWra (None, 2028)              1         \n",
      "_________________________________________________________________\n",
      "quant_dense_2 (QuantizeWrapp (None, 10)                20295     \n",
      "=================================================================\n",
      "Total params: 20,448\n",
      "Trainable params: 20,410\n",
      "Non-trainable params: 38\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:AutoGraph could not transform <tensorflow_model_optimization.python.core.quantization.keras.quantizers.AllValuesQuantizer object at 0x7ff4f29f8a30> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <tensorflow_model_optimization.python.core.quantization.keras.quantizers.AllValuesQuantizer object at 0x7ff4f29f8a30> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <tensorflow_model_optimization.python.core.quantization.keras.quantizers.AllValuesQuantizer object at 0x7ff4f29f8a30> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <tensorflow_model_optimization.python.core.quantization.keras.quantizers.AllValuesQuantizer object at 0x7ff4f29f8a30> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.1806 - accuracy: 0.9552WARNING:tensorflow:AutoGraph could not transform <tensorflow_model_optimization.python.core.quantization.keras.quantizers.AllValuesQuantizer object at 0x7ff4f29f8a30> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <tensorflow_model_optimization.python.core.quantization.keras.quantizers.AllValuesQuantizer object at 0x7ff4f29f8a30> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "2/2 [==============================] - 4s 355ms/step - loss: 0.1784 - accuracy: 0.9550 - val_loss: 0.1705 - val_accuracy: 0.9500\n",
      "Baseline test accuracy: 0.9535999894142151\n",
      "Quant test accuracy: 0.30880001187324524\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize the input image so that each pixel value is between 0 to 1.\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Define the model architecture.\n",
    "model = keras.Sequential([\n",
    "  keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "  keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "  keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
    "  keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "  keras.layers.Flatten(),\n",
    "  keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Train the digit classification model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(\n",
    "  train_images,\n",
    "  train_labels,\n",
    "  epochs=1,\n",
    "  validation_split=0.1,\n",
    ")\n",
    "\n",
    "quantize_model = tfmot.quantization.keras.quantize_model\n",
    "\n",
    "# q_aware stands for for quantization aware.\n",
    "q_aware_model = quantize_model(model)\n",
    "\n",
    "# `quantize_model` requires a recompile.\n",
    "q_aware_model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "q_aware_model.summary()\n",
    "\n",
    "train_images_subset = train_images[0:1000] # out of 60000\n",
    "train_labels_subset = train_labels[0:1000]\n",
    "\n",
    "q_aware_model.fit(train_images_subset, train_labels_subset,\n",
    "                  batch_size=500, epochs=1, validation_split=0.1)\n",
    "\n",
    "_, baseline_model_accuracy = model.evaluate(\n",
    "    test_images, test_labels, verbose=0)\n",
    "\n",
    "_, q_aware_model_accuracy = q_aware_model.evaluate(\n",
    "   test_images, test_labels, verbose=0)\n",
    "\n",
    "print('Baseline test accuracy:', baseline_model_accuracy)\n",
    "print('Quant test accuracy:', q_aware_model_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0c66fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_3 (Reshape)          (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "quant_conv2d_3 (QuantizeWrap (None, 26, 26, 12)        147       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 13, 13, 12)        0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 2028)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                20290     \n",
      "=================================================================\n",
      "Total params: 20,437\n",
      "Trainable params: 20,410\n",
      "Non-trainable params: 27\n",
      "_________________________________________________________________\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 2.3146 - accuracy: 0.0718 - val_loss: 2.2826 - val_accuracy: 0.1200\n",
      "Quant test accuracy: 0.13989999890327454\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize the input image so that each pixel value is between 0 to 1.\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "quantize_annotate_layer = tfmot.quantization.keras.quantize_annotate_layer\n",
    "\n",
    "# Define the model architecture.\n",
    "model = keras.Sequential([\n",
    "  keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "  keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "  quantize_annotate_layer(keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu')),\n",
    "  keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "  keras.layers.Flatten(),\n",
    "  keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "quantized_model = tfmot.quantization.keras.quantize_apply(model)\n",
    "\n",
    "\n",
    "# `quantize_model` requires a recompile.\n",
    "quantized_model.compile(optimizer='adam',\n",
    "               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "quantized_model.summary()\n",
    "\n",
    "train_images_subset = train_images[0:1000] # out of 60000\n",
    "train_labels_subset = train_labels[0:1000]\n",
    "\n",
    "quantized_model.fit(train_images_subset, train_labels_subset,\n",
    "                  batch_size=500, epochs=1, validation_split=0.1)\n",
    "\n",
    "h5_file = \"qat_single_layer.h5\"\n",
    "quantized_model.save(h5_file)\n",
    "\n",
    "_, q_aware_model_accuracy = quantized_model.evaluate(test_images, test_labels, verbose=0)\n",
    "\n",
    "print('Quant test accuracy:', q_aware_model_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2035d791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_3 (Reshape)          (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "quant_conv2d_3 (QuantizeWrap (None, 26, 26, 12)        147       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 13, 13, 12)        0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 2028)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                20290     \n",
      "=================================================================\n",
      "Total params: 20,437\n",
      "Trainable params: 20,410\n",
      "Non-trainable params: 27\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x7f7a4bf570d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 1s 379ms/step - loss: 2.3161 - accuracy: 0.0659 - val_loss: 2.2813 - val_accuracy: 0.1000\n",
      "Quant test accuracy: 0.13420000672340393\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize the input image so that each pixel value is between 0 to 1.\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "quantize_annotate_layer = tfmot.quantization.keras.quantize_annotate_layer\n",
    "\n",
    "in_layer = tf.keras.Input(shape=(28,28))\n",
    "dense = quantize_annotate_layer(tf.keras.layers.Dense(10))(in_layer)\n",
    "out_layer = tf.keras.layers.Flatten()(dense)\n",
    "\n",
    "annotated_model = tf.keras.Model(inputs=in_layer, outputs=out_layer)\n",
    "\n",
    "quantized_model = tfmot.quantization.keras.quantize_apply(annotated_model)\n",
    "\n",
    "quantized_model.summary()\n",
    "\n",
    "# `quantize_model` requires a recompile.\n",
    "quantized_model.compile(optimizer='adam',\n",
    "               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "quantized_model.fit(train_images_subset, train_labels_subset,\n",
    "                  batch_size=500, epochs=1, validation_split=0.1)\n",
    "\n",
    "h5_file = \"qat_test.h5\"\n",
    "quantized_model.save(h5_file)\n",
    "\n",
    "train_images_subset = train_images[0:1000] # out of 60000\n",
    "train_labels_subset = train_labels[0:1000]\n",
    "\n",
    "_, q_aware_model_accuracy = quantized_model.evaluate(test_images, test_labels, verbose=0)\n",
    "\n",
    "print('Quant test accuracy:', q_aware_model_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780248a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b87bb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
